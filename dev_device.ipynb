{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeviceEngine Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dedicated engine for device data, inherited from Core Engine. Each DeviceEngine class object will represent a unique device with its own set of processing parameters and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '[' (DeviceEngine.py, line 411)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\PC0118\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\n\u001b[1;33m    from src.StreamPort.device.DeviceEngine import DeviceEngine\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\PC0118\\Desktop\\StreamPort\\src\\StreamPort\\device\\DeviceEngine.py:411\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f'cleaning data if type is str. Type: {type(pressure_file['Time'].max())}')\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: unmatched '['\n"
     ]
    }
   ],
   "source": [
    "from src.StreamPort.device.DeviceEngine import DeviceEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtractedLabelled contains the simulated errors from the 2D-LC. Batches titled 'Basis' and 'Basis_post' are considered \"true\" curves, i.e., they represent standard conditions with no faults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify path to get analyses from\n",
    "base_dir = r'C:\\Users\\PC0118\\Desktop\\ExtractedSignals'\n",
    "#base_dir = r'C:\\Users\\PC0118\\Desktop\\ExtractedLabelled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an empty DeviceEngine object and prints it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dev = DeviceEngine(source = base_dir)\n",
    "dev.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeviceEngine object without an explicitly provided source performs all capabilities on files within the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev1 = DeviceEngine()\n",
    "#dev1.print()\n",
    "#print(dev1._source)\n",
    "#del dev1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProjectHeaders Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.core import ProjectHeaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = ProjectHeaders.ProjectHeaders(dtype = '2-D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(head.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add project headers. They can be passed as ProjectHeaders objects or dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_headers(headers = {'name': 'Pressure Curve Analysis', 'author': 'Sandeep H.'})\n",
    "dev.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeviceAnalysis Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each DeviceAnalysis object is a child of the Analysis Class. It holds the details of an Analysis for each individual device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.StreamPort.device.DeviceAnalysis import DeviceAnalysis\n",
    "\n",
    "#Creates an empty DeviceAnalysis object and prints it\n",
    "#devAnalysis = DeviceAnalysis()\n",
    "#devAnalysis.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "DeviceEngine's find_analyses() method returns a DeviceAnalysis Object or a list of DeviceAnalysis objects, besides printing the dataframes for each unique Method, paired with the metadata(Date, Runtime) for each curve.\n",
    "\n",
    "This method makes use of the source variable to accept a path to a directory containing analyses as an argument and find analyses from the target path.\n",
    "\n",
    "The path can refer to a directory containing data for specific groups of experiments \"210812_Gem 2021-08-12 09-49-10\" or one such experiment containing its own set of method-related analysis data \"210812_Gem--005.D\", \"210812_Gem--007.D\", ..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read analysis objects from engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses = dev.find_analyses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each DeviceEngine object has an attribute _method_ids that records all methods encountered in the analysis of the current Device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev._method_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an attribute _history to hold data on all experiments related to this device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add analyses objects that were found using find_analyses() to current device records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add analyses in the form of individual DeviceAnalysis objects or a list of such objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_analyses(analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ana = dev.get_analyses('09:59:42')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_analyses always returns a list, even if it contains only one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ana[0].print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the above analysis from the labeled data (09:59:42) slightly deviates from the remaining 'basis' data it is removed from the dataset, just as with the extreme anomalies(001-blank) so the training data is not polluted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.remove_analyses('Device Pressure Analysis - 240930_Mix-1_training-data_basis_post 2024-09-30 09-58-37| Start time: 09:59:42 09/30/24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeviceEngine's *plot_analyses()* and *plot_results()* calls each analysis object's respective *plot()* function after dynamically grouping related analyses. \n",
    "Grouping is done on the basis of unique method id's paired with unique experiment dates.\n",
    "User can set the 'group_by'(str) argument to control how the data is grouped. Defaults to 'method', otherwise 'date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot analyses by calling inbuilt plot function and passing each object's index as argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot analyses by word or subword present in analysis date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all available analyses by omitting 'analyses' argument.\n",
    "Group by defaults to 'method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.plot_analyses('basis', group_by='method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.plot_analyses(group_by='method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProcessingSettings - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new ProcessingSettings object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.device.DeviceProcSettings import ExtractPressureFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'weighted' argument of ExtractPressureFeatures object can be used to control whether the pressure curves should first be transformed by calculating percentage change between adjacent datapoints.\n",
    "Defaults to False, in which case feature extraction is performed on the raw pressure curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = ExtractPressureFeatures(weighted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add processing settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_settings(settings)\n",
    "dev.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the settings to extract pressure features after adding analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure_features = settings.run(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pressure_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the extracted features to the results (dict) attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_results(pressure_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the stored results associated with the current object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProcessingSettings - Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new ProcessingSettings object to extract seasonal components from analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.device.DeviceProcSettings import DecomposeCurves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*'period' argument of DecomposeCurves is used to control the window size over which the features are calculated. Defaults to 30 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_decompose = DecomposeCurves(period=30)#period was 30, defaults to 10. try both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_settings(curve_decompose)\n",
    "dev.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_components = curve_decompose.run(dev)\n",
    "print(seasonal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_results(seasonal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.get_results(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Each .D folder is an analysis with timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latest entry in analyses contains most up to date results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProcessingSettings - Fourier Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new ProcessingSettings object to perform Fast Fourier Analysis on raw curve and seasonal component of analyses time decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.device.DeviceProcSettings import FourierTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_transform = FourierTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_settings(fourier_transform)\n",
    "dev.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_seasonal = fourier_transform.run(dev)\n",
    "print(transformed_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_results(transformed_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.get_results(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled results are unavailable since data has not been scaled yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding features before scaling:\n",
    "scale_features() calls add_extracted_features() before grouping and scaling data.\n",
    "\n",
    "add_extracted_features() introduces new features that were extracted from the behaviour of the seasonal and noise components of the raw curves in the frequency domain. These frequencies were binned and averaged in different time-windows and added as features.\n",
    "\n",
    "Additional features added were Idle time of the batch, error in defined vs. measured runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProcessingSettings - Feature Scaling  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale extracted and engineered features to improve the quality of the information we get from them. These prove more useful when visually analysing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.device.DeviceProcSettings import Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User selects the type of scaler to be used from preloaded options : 'minmax', 'std'(Standard), 'robust', 'maxabs', 'norm'(Normalizer).\n",
    "Scaler defualts to Normalizer in the absence of an argument.\n",
    "\n",
    "'replace' argument allows user to replace existing features with scaled features or to create a new entry instead. Defaults to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = Scaler(parameters='robust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_settings(feature_scaler)\n",
    "dev.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = feature_scaler.run(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.add_results(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the computed results of feature extraction for chosen results based on user input to select *base* to extract base features, *decompose* for seasonal decomposition, fourier *transform* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User may also plot the raw pressure curves by omitting the 'features' argument, indicating that the *results* of feature extraction are not to be plotted, just the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this_method = 'Pac' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'group_by' allows user to group data either by 'date' or 'method':\n",
    "1. 'date' prepares data with weight on experiment date. So matching methods on different dates will not be grouped.\n",
    "2. 'method' prepares data purely on method and groups all available data for the given method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.plot_results(this_method, group_by='method', interactive=True, scaled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select features to plot. Setting 'scaled' argument allows to toggle plots of scaled features or unscaled. Defaults to True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.plot_results(results = 'Pac', features ='base', scaled=True, transpose=True, group_by='method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.plot_results(results = ['11:35:38', '15:53:24'], features ='base', transpose=True, interactive=True, scaled=True, type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 'interactive' argument to toggle between static and interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.plot_results(results = ['11:35:38', '15:53:24'], features ='decompose', scaled=False, interactive= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting type to 'box' enables a box plot of the data. Available options are 'box' and 'scatter' by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev.plot_results(results = ['11:35:38', '15:53:24'], features ='transform', scaled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MachineLearning - Isolation Forest for preliminary classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD CLASS LABELS TO ANALYSIS OBJECTS AFTER FEATURE ANALYSIS. FIRST ANALYSIS '001-blank' is assigned a separate class of ML operations due to it being a systematic fault."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify() dynamically assigns class labels through MLEngine's make_iso_forest() to all analyses encountered and classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a MachineLearningEngine object to enable ML ops on prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative to running iso_forest to create each engine object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_device_data from each MLEngine object can extract relevant and preprocessed data from a given device, while also updating it to conform to MLAnalysis object specifications.\n",
    "Iso forest was designed to have this method inbuilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_engine = MachineLearningEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_state(int) argument can be specified to reproduce results. Defaults to None, sets a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelIsoForest\n",
    "iso_forest = MakeModelIsoForest(dev, random_state=22)#22 seemed to pick better train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_engine.add_settings(iso_forest)\n",
    "ml_engine.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_objects = iso_forest.run(ml_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_engine = method_objects[-2][0]\n",
    "points = pac_engine.get_data()\n",
    "pac_anomalies = pac_engine.get_anomalies()\n",
    "print('dataset', points)\n",
    "print('anomalies:', pac_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomalies = []\n",
    "for i in range(len(method_objects)):\n",
    "    engine = method_objects[i][0]\n",
    "    anomalies = engine.get_anomalies()\n",
    "    anomalies.to_csv(f'anomalies_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_preds = method_objects[-2][1]\n",
    "print('target var', pac_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = points\n",
    "y = pac_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics - Cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest as iso\n",
    "model = iso(contamination= 0.15, bootstrap= True, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_scores = cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MachineLearning - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_iso_forest() of MLEngine class automatically creates sub-objects of MLEngine class for each encountered group of analyses per unique method after performing iso_forest and plotting results. Can be modified to save results later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is first used in dimensionality reduction before applying DBSCAN and also as an alternative outlier detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelPCASKL\n",
    "pca = MakeModelPCASKL(n_components = 2, center_data= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pac_engine = method_engines['Pac_engine']\n",
    "#ikali_engine = method_engines['Irino_Kali_engine']\n",
    "\n",
    "pac_engine.add_settings(pca)\n",
    "pca_scores = pca.run(pac_engine)\n",
    "pac_engine.add_results(pca_scores)\n",
    "pca_results = pac_engine.get_results('pca_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Principal components:\\n\", pca_results[1].components_)\n",
    "print(\"Explained variance ratio:\", pca_results[1].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed data:\\n\", pca_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.plot_results('basis_post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.plot_results(results ='basis_post', features ='base', scaled=True, transpose=False, group_by='method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.plot_results(results ='basis_post', features ='base', scaled=True, transpose=True, group_by='method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_pca = pca_results[0]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], color='blue', label='Data Points')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Score Plot')\n",
    "\n",
    "sample_names = list(points.index)\n",
    "samples = [(point.split('|')[-1]).split(' ')[-2] for point in sample_names]\n",
    "\n",
    "print(len(x_pca))\n",
    "# Optional: Add text annotations for each point\n",
    "for i in range(len(x_pca)):\n",
    "    plt.text(x_pca[i, 0], x_pca[i, 1], samples[i], fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optionally print the explained variance\n",
    "print(\"Explained variance ratio:\", pca_results[1].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "eps = 0.8 # Adjust based on the k-distance plot\n",
    "min_samples = 6 # Adjust based on your data\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan_results = dbscan.fit_predict(x_pca)\n",
    "print(dbscan_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=dbscan_results, cmap='viridis', marker='o', s=50)\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Partial Least Squares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial least sqares is used as an additional metric to identify anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Step 3: Fit PLS regression model (let's choose 2 components)\n",
    "pls = PLSRegression(n_components=2)\n",
    "y = [i for i in range(0, len(samples))]\n",
    "pls.fit(points, dbscan_results)# try y, gives different plot, keep looking into it\n",
    "\n",
    "# Step 4: Project data onto PLS components\n",
    "X_pls = pls.transform(points)\n",
    "\n",
    "# Step 5: Plot the PLS components\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the first PLS component vs the second PLS component\n",
    "plt.scatter(X_pls[:, 0], X_pls[:, 1], c=dbscan_results, cmap='viridis', edgecolor='k', s=100)\n",
    "plt.colorbar(label='Target Variable (y)')\n",
    "plt.title('PLS Components Plot')\n",
    "plt.xlabel('PLS Component 1')\n",
    "plt.ylabel('PLS Component 2')\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "# Step 6: Plot explained variance (scree plot)\n",
    "explained_variance = pls.x_scores_.var(axis=0) / np.var(points, axis=0).sum()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7)\n",
    "plt.xlabel('PLS Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.title('Explained Variance by PLS Components')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - One-Class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of One-Class SVM for anomaly detection. Most appropriate when ONLY normal data is available to find anomalies in new unlabeled data. Try with Kjell's labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Train One-Class SVM on the normal data (you can use only normal data for training if you have separate normal data) \n",
    "svm = OneClassSVM(nu=0.1, kernel='rbf', gamma=0.0001)# nu controls the number of outliers\n",
    "X_scaled = points\n",
    "svm.fit(x_pca)\n",
    "\n",
    "# Predict anomalies\n",
    "y_pred = svm.predict(x_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dbscan_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot normal points (predicted as 1)\n",
    "plt.scatter(x_pca[y_pred == 1][:, 0], x_pca[y_pred == 1][:, 1], color='blue', label='Normal')\n",
    "\n",
    "# Plot outliers (predicted as -1)\n",
    "plt.scatter(x_pca[y_pred == -1][:, 0], x_pca[y_pred == -1][:, 1], color='red', label='Anomalies')\n",
    "\n",
    "plt.title(\"One-Class SVM for Anomaly Detection (Multiple Features)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics - Precision score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics like Precision, F1, Recall score will be used here to check effectivity of model at detecting anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to evaluate model through cross-validation and evalutation metrics\n",
    "#from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification metrics\n",
    "#precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "#print(f\"Precision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics - Recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall = recall_score(y_test, y_pred)\n",
    "\n",
    "#print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics - F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "#print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce everything here on Orange and then try 26k data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML file parsing for real-time classification and maintenance - Bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bs4 using html.parser can parse malformed xml unusable with ET. Future implementation will allow to scan for actuals in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_ordner = r'C:\\Users\\PC0118\\Desktop\\actuals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filegroup = os.listdir(xml_ordner)\n",
    "num_files = len(filegroup)\n",
    "print(num_files)\n",
    "for f in filegroup:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filegroup.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = filegroup[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(xml_ordner, file), 'r') as f:\n",
    "        content = f.read()                                                                \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the malformed XML with BeautifulSoup\n",
    "soup = bs(content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())  # View the prettified XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML parsing - ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_dfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverse from root to end nodes and find relevant status information to build a dataframe out of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50, 51):\n",
    "        try:  \n",
    "\n",
    "                tree = ET.parse(os.path.join(xml_ordner, filegroup[30]))\n",
    "                root = tree.getroot()\n",
    "                #second child of root contains actuals, first child holds schematics for data\n",
    "                #print(root[0].tag, root[0].attrib)\n",
    "                diffgrams = root[1]\n",
    "                #print(len(diffgrams))\n",
    "                #print(diffgrams.tag, diffgrams.attrib, diffgrams.text)\n",
    "\n",
    "\n",
    "                \n",
    "                #dataframe to hold data in xml file initiated with list of entries and sample names identified by timestamp \n",
    "                diffgram_df = pd.DataFrame()\n",
    "                samples = []\n",
    "\n",
    "                feature = []\n",
    "\n",
    "                num_observations = len(diffgrams[0])\n",
    "                print(f\"Actuals file: {filegroup[30]} - No fatal errors, num. observations: {num_observations}\")\n",
    "\n",
    "                for element in diffgrams[0]:\n",
    "                        #print(element.tag, element.attrib)#, element[0].text\n",
    "\n",
    "                        if element[0].text in samples:\n",
    "                                feature = pd.DataFrame(feature, index=[f'Analysis - {sample}' for sample in samples])\n",
    "                                diffgram_df = pd.concat([diffgram_df, feature], axis = 1)\n",
    "                                feature = []\n",
    "                                samples = []\n",
    "                        feature.append({element.tag : element.get('{urn:schemas-microsoft-com:xml-diffgram-v1}id')})\n",
    "                        samples.append(element[0].text)\n",
    "                if feature != [] or samples != []:\n",
    "                        feature = pd.DataFrame(feature, index=[f'Analysis - {sample}' for sample in samples])\n",
    "                        diffgram_df = pd.concat([diffgram_df, feature], axis = 1)\n",
    "\n",
    "                good_dfs.update({file : diffgram_df})\n",
    "                #print(diffgram_df)\n",
    "\n",
    "        except Exception as e:\n",
    "                bad_files.append({'file' : filegroup[30],\n",
    "                                  'error': e})\n",
    "                print(f\"Actuals file: {filegroup[30]} - Error encountered: {e}. Skipping this iteration for file number {35}.\")\n",
    "                continue\n",
    "\n",
    "print('files read:', 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(good_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = good_dfs['actuals 23.9.2024 8_30_22-252.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[12800:12850, :]\n",
    "df2 = df.iloc[26220:26270, :]\n",
    "df3 = df.iloc[28000:28040, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure to hold the table\n",
    "fig, ax = plt.subplots(figsize=(6, 2))  # Adjust size as needed\n",
    "ax.axis('off')  # Hide axes\n",
    "tbl = table(ax, df1, loc='center', colWidths=[0.2]*df1.shape[1])  # Adjust column width if needed\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(12)\n",
    "tbl.scale(1.2, 1.2)  # Scale the table\n",
    "\n",
    "# Save the table as an image\n",
    "image_path = 'table_image.png'\n",
    "plt.savefig(image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages to create a dashboard\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html \n",
    "from dash.dependencies import Input, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up divisions with the option to select the information to be displayed\n",
    "\n",
    "# something off here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "app.layout =html.Div([\n",
    "\n",
    "                        html.Div([  \n",
    "                        html.H1('Title', style={'text-align' : 'center'}),\n",
    "\n",
    "\n",
    "                        dcc.RadioItems(\n",
    "                                        id='radio-items',\n",
    "                                        options=[\n",
    "                                                    {'label' : 'Curves', 'value' : ''},\n",
    "                                                    {'label' : 'Features', 'value' : 'base'},\n",
    "                                                    {'label' : 'Decomp', 'value' : 'decompose'},\n",
    "                                                    {'label' : 'Transform', 'value' : 'transform'}\n",
    "                                                ],\n",
    "                                        value=''   #default\n",
    "                                       ),\n",
    "                                       html.Div(id='output-container',\n",
    "                                                style={\n",
    "                                                    'backgroundColor': '#f9f9f9',\n",
    "                                                    'border': '1px solid #ccc',\n",
    "                                                    'padding': '20px',\n",
    "                                                    'borderRadius': '5px',\n",
    "                                                    'boxShadow': '2px 2px 12px rgba(0, 0, 0, 0.1)'\n",
    "                                                    }\n",
    "                                                )\n",
    "                                ]),\n",
    "\n",
    "                        html.Div([\n",
    "                        dcc.DatePickerRange(\n",
    "                            id='date-picker-range',\n",
    "                            start_date='2023-01-01',\n",
    "                            end_date='2023-12-31',\n",
    "                            display_format='YYYY-MM-DD'\n",
    "                        )\n",
    "                        ], style={'border': '1px solid black', 'padding': '10px', 'margin': '10px'})\n",
    "\n",
    "                    ]) \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import webbrowser\n",
    "@app.callback(    \n",
    "    Output('output-container', 'children'),\n",
    "    Input('radio-items', 'value')\n",
    ")\n",
    "def update_graph(value):\n",
    "    dev.plot_results('Pac', features=value)\n",
    "    #webbrowser.open('plot.html')\n",
    "    return     \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
