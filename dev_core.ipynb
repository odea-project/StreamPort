{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoreEngine Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoreEngine class serves as parent to data dedicated Engine classes (e.g. MachineLearningEngine), following inheritance. Similar methods accros engines are applied to the CoreEngine for reusability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.core.CoreEngine import CoreEngine\n",
    "\n",
    "# Creates an empty CoreEngine object and prints it\n",
    "x = CoreEngine()\n",
    "x.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProjectHeaders Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ProjectHeaders class is used for managment of project information when using an Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.core.CoreEngine import CoreEngine\n",
    "from src.StreamPort.core.ProjectHeaders import ProjectHeaders\n",
    "\n",
    "x = CoreEngine()\n",
    "\n",
    "print(\"1:\")\n",
    "# Creates a ProjectHeaders object and prints it \n",
    "ph = ProjectHeaders()\n",
    "ph.print()\n",
    "\n",
    "# Validates the ProjectHeaders object\n",
    "print(ph.validate())\n",
    "\n",
    "print(\"\\n2:\")\n",
    "# Modifies headers in the CoreEngine using a dictionary\n",
    "x.add_headers(headers={\"name\": \"Example Name\", \"description\": \"demo description\"})\n",
    "x.print()\n",
    "\n",
    "print(x.get_headers())\n",
    "\n",
    "print(\"\\n3:\")\n",
    "# Removes the description header\n",
    "x.remove_headers(\"description\")\n",
    "print(x.get_headers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the engine, data is treated as units of class Analysis. Analysis is always the parent of sub-class dedicated childs (e.g. SensorAnalysis). The Analysis holds similar methods and attributes to all Analysis class childs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Creates an Analysis object and prints it\n",
      "\n",
      "Analysis\n",
      "  name: Analysis 1\n",
      "  replicate: None\n",
      "  blank: None\n",
      "  data:\n",
      "    x (size 10)\n",
      "    y (size 10)\n",
      "\n",
      "Data: \n",
      "x: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "y: [39 48 21 47 13 79 79 23 35 88]\n",
      "\n",
      "\n",
      "2: Creates a list of Analysis objects and prints the number of analyses in the list\n",
      "Number of analyses:  3\n",
      "\n",
      "\n",
      "3: Adds the list of analyses to the CoreEngine\n",
      "Number of analyses:  3\n",
      "\n",
      "\n",
      "4: Gets the second analysis from the engine\n",
      "\n",
      "Analysis\n",
      "  name: Analysis 2\n",
      "  replicate: None\n",
      "  blank: None\n",
      "  data:\n",
      "    x (size 10)\n",
      "    y (size 10)\n",
      "\n",
      "5: Removes the first analysis from the engine\n",
      "Number of analyses:  2\n",
      "\n",
      "\n",
      "6: Removes all analyses from the engine\n",
      "Number of analyses:  0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.StreamPort.core.CoreEngine import CoreEngine\n",
    "from src.StreamPort.core.Analysis import Analysis\n",
    "import numpy as np\n",
    "\n",
    "x = CoreEngine()\n",
    "\n",
    "print(\"1: Creates an Analysis object and prints it\")\n",
    "\n",
    "ana1 = Analysis(name = \"Analysis 1\", data = {\"x\": np.arange(1, 11), \"y\": np.random.randint(0, 100, 10)})\n",
    "ana1.print()\n",
    "\n",
    "print(\"Data: \")\n",
    "for key, value in ana1.data.items():\n",
    "  print(f\"{key}: {value}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"2: Creates a list of Analysis objects and prints the number of analyses in the list\")\n",
    "anaList = [\n",
    "  ana1,\n",
    "  Analysis(name = \"Analysis 2\", data = {\"x\": np.arange(1, 11), \"y\": np.random.randint(0, 100, 10)}),\n",
    "  Analysis(name = \"Analysis 3\", data = {\"x\": np.arange(1, 11), \"y\": np.random.randint(0, 100, 10)})\n",
    "]\n",
    "print(\"Number of analyses: \", anaList.__len__())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"3: Adds the list of analyses to the CoreEngine\")\n",
    "x.add_analyses(anaList)\n",
    "print(\"Number of analyses: \", x._analyses.__len__())\n",
    "print(\"/n\")\n",
    "\n",
    "print(\"4: Gets the second analysis from the engine\")\n",
    "ana2 = x.get_analyses(1)\n",
    "print(ana2)\n",
    "\n",
    "print(\"5: Removes the first analysis from the engine\")\n",
    "x.remove_analyses(0)\n",
    "print(\"Number of analyses: \", x._analyses.__len__())\n",
    "print(\"/n\")\n",
    "\n",
    "print(\"6: Removes all analyses from the engine\")\n",
    "x.remove_analyses()\n",
    "print(\"Number of analyses: \", x._analyses.__len__())\n",
    "print(\"/n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProcessingSettings Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ProcessingSettings class is used to assemble data procesing wokflows within the each engine. In the engine, the ProcessingSettings are stored as a list and the order of the ProcessingSettings dictate the order of the data processing workflow to be applied to the data in each analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Creates a ProcessingSettings object and prints it\n",
      "\n",
      " ProcessingSettings\n",
      " call         None\n",
      " algorithm    None\n",
      " version      None\n",
      " software     None\n",
      " developer    None\n",
      " contact      None\n",
      " link         None\n",
      " doi          None\n",
      "\n",
      " parameters: empty \n",
      "\n",
      "2: Adds the settings to the CoreEngine\n",
      "Number of settings:  1\n",
      "\n",
      "\n",
      "3: Removes the settings from the CoreEngine\n",
      "Number of settings:  0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.StreamPort.core.CoreEngine import CoreEngine\n",
    "from src.StreamPort.core.ProcessingSettings import ProcessingSettings\n",
    "\n",
    "x = CoreEngine()\n",
    "\n",
    "print(\"1: Creates a ProcessingSettings object and prints it\")\n",
    "settings = ProcessingSettings()\n",
    "settings.print()\n",
    "\n",
    "print(\"2: Adds the settings to the CoreEngine\")\n",
    "x.add_settings(settings)\n",
    "print(\"Number of settings: \", x._settings.__len__())\n",
    "print(\"/n\")\n",
    "\n",
    "print(\"3: Removes the settings from the CoreEngine\")\n",
    "x.remove_settings()\n",
    "print(\"Number of settings: \", x._settings.__len__())\n",
    "print(\"/n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProcessingSettings method dispatchment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ProcessingSettings method dispatchment is used to apply the workflow based on class hierarchy. Below, an implementation example of a class processing method and possible algorithms is given. From a processing method run, results objects are always returned. The results are then updated in the engine. When results are already present the results in the engine are used to process and not the raw data within each analysis. See the results structure in the subchapter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.core.CoreEngine import CoreEngine\n",
    "from src.StreamPort.core.ProcessingSettings import ProcessingSettings\n",
    "from src.StreamPort.core.Analysis import Analysis\n",
    "import numpy as np\n",
    "\n",
    "# Processing method specific class\n",
    "class NormalizeData(ProcessingSettings):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.call = \"normalize_data\"\n",
    "\n",
    "  def run(self):\n",
    "    pass\n",
    "\n",
    "# Algorithm specific class\n",
    "class NormalizeDataMinMax(NormalizeData):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.algorithm = \"min_max\"\n",
    "  \n",
    "  def run(self, engine):\n",
    "    if engine._results.__len__() > 0:\n",
    "      results = engine._results\n",
    "    else:\n",
    "      results = {}\n",
    "      for analysis in engine._analyses:\n",
    "        if isinstance(analysis.data, dict) and 'y' in analysis.data:\n",
    "          results.update({analysis.name: analysis.data})\n",
    "        else:\n",
    "          print(f\"Skipping {analysis.name} because its data is not a dictionary with a 'y' key.\")\n",
    "\n",
    "    for key in results:\n",
    "      y = np.array(results[key]['y'])\n",
    "      norm_data = (y - y.min()) / (y.max() - y.min())\n",
    "      results[key].update({'y': norm_data})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Algorithm specific class\n",
    "class NormalizeDataSNV(NormalizeData):\n",
    "  def __init__(self, liftToZero = True):\n",
    "    super().__init__()\n",
    "    self.algorithm = \"standard_variance_normalization\"\n",
    "    self.parameters = {\"liftToZero\": liftToZero}\n",
    "  \n",
    "  def run(self, engine):\n",
    "    if engine._results.__len__() > 0:\n",
    "      results = engine._results\n",
    "    else:\n",
    "      results = {}\n",
    "      for analysis in engine._analyses:\n",
    "        if isinstance(analysis.data, dict) and 'y' in analysis.data:\n",
    "          results.update({analysis.name: analysis.data})\n",
    "        else:\n",
    "          print(f\"Skipping {analysis.name} because its data is not a dictionary with a 'y' key.\")\n",
    "\n",
    "    for key in results:\n",
    "      y = np.array(results[key]['y'])\n",
    "      norm_data = (y - y.mean()) / y.std()\n",
    "\n",
    "      if self.parameters[\"liftToZero\"]:\n",
    "        norm_data += abs(norm_data.min())\n",
    "\n",
    "      results[key].update({'y': norm_data})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create an example analysis list\n",
    "ex_analyses = [\n",
    "  Analysis(name = \"Analysis 1\", data = {\"x\": np.arange(1, 11), \"y\": np.random.randint(0, 100, 10)}),\n",
    "  Analysis(name = \"Analysis 2\", data = {\"x\": np.arange(1, 11), \"y\": np.random.randint(0, 100, 10)})\n",
    "]\n",
    "\n",
    "x = CoreEngine(analyses = ex_analyses)\n",
    "\n",
    "print(\"1: Prints the analyses in the CoreEngine\")\n",
    "for analysis in x._analyses:\n",
    "  print(f\"Analysis: {analysis.name}\")\n",
    "  for key, value in analysis.data.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "  print(\"/n\")\n",
    "\n",
    "# Create an instance of the NormalizeDataMinMax class\n",
    "norm_settings = NormalizeDataMinMax()\n",
    "\n",
    "# Create an instance of the NormalizeDataSNV class\n",
    "norm_settings2 = NormalizeDataSNV()\n",
    "\n",
    "# Run the settings MinMax using the engine as argument\n",
    "results = norm_settings.run(x)\n",
    "\n",
    "print(\"2: Results from settings MinMax\")\n",
    "for key in results:\n",
    "  print(f\"Analysis: {key}\")\n",
    "  print(f\"y: {results[key]['y']}\")\n",
    "  print(\"\\n\")\n",
    "\n",
    "# Run the settings SNV using the engine as argument\n",
    "results2 = norm_settings2.run(x)\n",
    "\n",
    "print(\"3: Results from settings snv\")\n",
    "for key in results2:\n",
    "  print(f\"Analysis: {key}\")\n",
    "  print(f\"y: {results2[key]['y']}\")\n",
    "  print(\"\\n\")\n",
    "\n",
    "# Adds settings to the CoreEngine\n",
    "x.add_settings(norm_settings)\n",
    "\n",
    "x.print()\n",
    "\n",
    "x.run_workflow()\n",
    "\n",
    "print(\"4: Results from the engine\")\n",
    "for key in x._results:\n",
    "  print(f\"Analysis: {key}\")\n",
    "  print(f\"y: {x._results[key]['y']}\")\n",
    "  print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processed the data is stored in Results class objects as a differentiation from Analysis class that refers to raw data. There is not defined class for results as they mirror the data dict attribute from each analysis, holding the modified/processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.core.CoreEngine import CoreEngine\n",
    "\n",
    "x = CoreEngine()\n",
    "\n",
    "print(\"1: Creates a results dictionary with data from two sensors\")\n",
    "res1 = {\"sensor1\": {\"x\": [1, 2, 3, 4, 5], \"y\": [1, 2, 3, 4, 5]}, \"sensor2\": {\"x\": [1, 2, 3, 4, 5], \"y\": [1, 2, 3, 4, 5]}}\n",
    "print(res1)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"2: Adds the results to the CoreEngine\")\n",
    "x.add_results(res1)\n",
    "print(\"Number of results: \", x._results.__len__())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"3: Removes the results in the CoreEngine\")\n",
    "x.remove_results(\"sensors\")\n",
    "print(\"Number of results: \", x._results.__len__())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Inheritance\n",
    "\n",
    "Engines are created based on the class inheritance as described below.\n",
    "Basic methods are reused from the core engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "  \"\"\"Class docstrings go here.\"\"\"\n",
    "\n",
    "  def __init__(self, fname, lname):\n",
    "    self.firstname = fname\n",
    "    self.lastname = lname\n",
    "\n",
    "  def printname(self):\n",
    "    \"\"\"Class method docstrings go here.\"\"\"\n",
    "    print(self.firstname, self.lastname)\n",
    "\n",
    "  def welcome_from_parent(self):\n",
    "    print(\"Welcome from parent class\")\n",
    "\n",
    "#Use the Person class to create an object, and then execute the printname method:\n",
    "\n",
    "x = Person(\"John\", \"Doe\")\n",
    "\n",
    "x.printname()\n",
    "\n",
    "class Student(Person):\n",
    "  def __init__(self, fname, lname, year):\n",
    "    super().__init__(fname, lname)\n",
    "    self.graduationyear = year\n",
    "\n",
    "  def welcome(self):\n",
    "    print(\"Welcome\", self.firstname, self.lastname, \"to the class of\", self.graduationyear)\n",
    "\n",
    "  def printname(self):\n",
    "    print(self.firstname, self.lastname, \"overwritten!\")\n",
    "\n",
    "  def printname_from_parent(self):\n",
    "    super().printname()\n",
    "\n",
    "x = Student(\"Mike\", \"Olsen\", 2019)\n",
    "\n",
    "x.printname()\n",
    "\n",
    "x.printname_from_parent()\n",
    "\n",
    "x.welcome()\n",
    "\n",
    "x.welcome_from_parent()\n",
    "\n",
    "#help(Person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Code Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/apoli/My Drive/submission_docs/20240829_Projektskizzen_IUTA_UDE_ZunA_final.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from PyPDF2 import PdfMerger\n",
    "\n",
    "# # Paths to the uploaded PDF files\n",
    "# pdf1_path = 'C:/Users/apoli/My Drive/submission_docs/20240829_Projektskizzen_IUTA_UDE_ZunA.pdf'\n",
    "# pdf2_path = 'C:/Users/apoli/My Drive/submission_docs/LOI_LINEG.pdf'\n",
    "# pdf3_path = 'C:/Users/apoli/My Drive/submission_docs/LOI_EGLV.pdf'\n",
    "# pdf4_path = 'C:/Users/apoli/My Drive/submission_docs/LOI_RV.pdf'\n",
    "# pdf5_path = 'C:/Users/apoli/My Drive/submission_docs/LOI_LW.pdf'\n",
    "\n",
    "# # Output path for the merged PDF\n",
    "# output_path = 'C:/Users/apoli/My Drive/submission_docs/20240829_Projektskizzen_IUTA_UDE_ZunA_final.pdf'\n",
    "\n",
    "# # Create a PdfMerger object\n",
    "# merger = PdfMerger()\n",
    "\n",
    "# # Append the PDFs\n",
    "# merger.append(pdf1_path)\n",
    "# merger.append(pdf2_path)\n",
    "# merger.append(pdf3_path)\n",
    "# merger.append(pdf4_path)\n",
    "# merger.append(pdf5_path)\n",
    "\n",
    "# # Write out the merged PDF\n",
    "# merger.write(output_path)\n",
    "# merger.close()\n",
    "\n",
    "# output_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
