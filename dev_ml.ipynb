{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MachineLearningEngine Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MachineLearningEngine class is builds on the CoreEngine class. The CoreEngine class serves as a parent class engines that focus on data, while the MachineLearningEngine class is for engines that focus on learning from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "engine = MachineLearningEngine()\n",
    "engine.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MachineLearningAnalysis Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MachineLearningAnalysis class is builds on the class Analysis. The Analysis class that is used to perform analysis on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningAnalysis import MachineLearningAnalysis\n",
    "\n",
    "#Creates an empty MachineLearningAnalysis obejct and prints it\n",
    "analysis = MachineLearningAnalysis()\n",
    "analysis.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CSV File  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method loads the dataset from csv file and create a list of analysis object. Used the data to make a matrix with the analysis names and visualizes the results using a scatter plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of the CSV file: {'number_of_rows': 45, 'number_of_columns': 4445}\n",
      "\n",
      "MachineLearningEngine \n",
      "  name: None \n",
      "  author: None \n",
      "  path: None \n",
      "  date: 2024-08-28 02:03:13.537510 \n",
      "  analyses: 45 \n",
      "  settings: 0 \n",
      "\n",
      "Create a list of analysis object and prints it\n",
      "Analysis: M200317000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200317007\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200317008\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200317009\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [   0.            0.         6121.72086589 ...    0.            0.\n",
      "    0.        ]\n",
      "\n",
      "\n",
      "Analysis: M200317010\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200318000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200318001\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200318002\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200318003\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200318004\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200320000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200320001\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200320002\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200320003\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200320004\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200401000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200401068\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [    0.             0.             0.         ...     0.\n",
      "     0.         19759.97135417]\n",
      "\n",
      "\n",
      "Analysis: M200401069\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200401070\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [   0.            0.            0.         ... 7663.00325521    0.\n",
      "    0.        ]\n",
      "\n",
      "\n",
      "Analysis: M200401071\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200403000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200403005\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200403006\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200403007\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200403008\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [     0.              0.              0.         ...      0.\n",
      " 376593.10416667      0.        ]\n",
      "\n",
      "\n",
      "Analysis: M200408000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200408025\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200408026\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [17037.33919271     0.             0.         ...     0.\n",
      "     0.             0.        ]\n",
      "\n",
      "\n",
      "Analysis: M200408027\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [   0.         6555.61523438    0.         ...    0.            0.\n",
      "    0.        ]\n",
      "\n",
      "\n",
      "Analysis: M200408028\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200414000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200414005\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200414006\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200414007\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [   0.            0.            0.         ... 7785.83024089    0.\n",
      "    0.        ]\n",
      "\n",
      "\n",
      "Analysis: M200414008\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200422000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200422146\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200422147\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200422148\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [   0.            0.            0.         ... 8125.63427734    0.\n",
      "    0.        ]\n",
      "\n",
      "\n",
      "Analysis: M200422149\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200429000A\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200429096\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200429097\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200429098\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysis: M200429099\n",
      "x: ['M100_R148_13791' 'M100_R178_13767' 'M100_R585_13804' ... 'M99_R97_13147'\n",
      " 'M993_R1888_113866' 'M993_R1893_113846']\n",
      "y: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "Analysename:  ['M200317000A', 'M200317007', 'M200317008', 'M200317009', 'M200317010', 'M200318000A', 'M200318001', 'M200318002', 'M200318003', 'M200318004', 'M200320000A', 'M200320001', 'M200320002', 'M200320003', 'M200320004', 'M200401000A', 'M200401068', 'M200401069', 'M200401070', 'M200401071', 'M200403000A', 'M200403005', 'M200403006', 'M200403007', 'M200403008', 'M200408000A', 'M200408025', 'M200408026', 'M200408027', 'M200408028', 'M200414000A', 'M200414005', 'M200414006', 'M200414007', 'M200414008', 'M200422000A', 'M200422146', 'M200422147', 'M200422148', 'M200422149', 'M200429000A', 'M200429096', 'M200429097', 'M200429098', 'M200429099']\n",
      "Matrix: \n",
      "              M100_R148_13791  M100_R178_13767  M100_R585_13804  \\\n",
      "M200317000A         0.000000         0.000000         0.000000   \n",
      "M200317007          0.000000         0.000000         0.000000   \n",
      "M200317008          0.000000         0.000000         0.000000   \n",
      "M200317009          0.000000         0.000000      6121.720866   \n",
      "M200317010          0.000000         0.000000         0.000000   \n",
      "M200318000A         0.000000         0.000000         0.000000   \n",
      "M200318001          0.000000         0.000000         0.000000   \n",
      "M200318002          0.000000         0.000000         0.000000   \n",
      "M200318003          0.000000         0.000000         0.000000   \n",
      "M200318004          0.000000         0.000000         0.000000   \n",
      "M200320000A         0.000000         0.000000         0.000000   \n",
      "M200320001          0.000000         0.000000         0.000000   \n",
      "M200320002          0.000000         0.000000         0.000000   \n",
      "M200320003          0.000000         0.000000         0.000000   \n",
      "M200320004          0.000000         0.000000         0.000000   \n",
      "M200401000A         0.000000         0.000000         0.000000   \n",
      "M200401068          0.000000         0.000000         0.000000   \n",
      "M200401069          0.000000         0.000000         0.000000   \n",
      "M200401070          0.000000         0.000000         0.000000   \n",
      "M200401071          0.000000         0.000000         0.000000   \n",
      "M200403000A         0.000000         0.000000         0.000000   \n",
      "M200403005          0.000000         0.000000         0.000000   \n",
      "M200403006          0.000000         0.000000         0.000000   \n",
      "M200403007          0.000000         0.000000         0.000000   \n",
      "M200403008          0.000000         0.000000         0.000000   \n",
      "M200408000A         0.000000         0.000000         0.000000   \n",
      "M200408025          0.000000         0.000000         0.000000   \n",
      "M200408026      17037.339193         0.000000         0.000000   \n",
      "M200408027          0.000000      6555.615234         0.000000   \n",
      "M200408028          0.000000         0.000000         0.000000   \n",
      "M200414000A         0.000000         0.000000         0.000000   \n",
      "M200414005          0.000000         0.000000         0.000000   \n",
      "M200414006          0.000000         0.000000         0.000000   \n",
      "M200414007          0.000000         0.000000         0.000000   \n",
      "M200414008          0.000000         0.000000         0.000000   \n",
      "M200422000A         0.000000         0.000000         0.000000   \n",
      "M200422146          0.000000         0.000000         0.000000   \n",
      "M200422147          0.000000         0.000000         0.000000   \n",
      "M200422148          0.000000         0.000000         0.000000   \n",
      "M200422149          0.000000         0.000000         0.000000   \n",
      "M200429000A         0.000000         0.000000         0.000000   \n",
      "M200429096          0.000000         0.000000         0.000000   \n",
      "M200429097          0.000000         0.000000         0.000000   \n",
      "M200429098          0.000000         0.000000         0.000000   \n",
      "M200429099          0.000000         0.000000         0.000000   \n",
      "\n",
      "             M100_R585_13975  M101_R866_14387  M102_R1939_14714  \\\n",
      "M200317000A         0.000000         0.000000          0.000000   \n",
      "M200317007          0.000000         0.000000          0.000000   \n",
      "M200317008          0.000000         0.000000          0.000000   \n",
      "M200317009      17925.878255         0.000000          0.000000   \n",
      "M200317010      11041.857096         0.000000          0.000000   \n",
      "M200318000A         0.000000         0.000000          0.000000   \n",
      "M200318001          0.000000         0.000000          0.000000   \n",
      "M200318002          0.000000         0.000000          0.000000   \n",
      "M200318003       8016.484375         0.000000          0.000000   \n",
      "M200318004       7999.371908         0.000000          0.000000   \n",
      "M200320000A         0.000000         0.000000          0.000000   \n",
      "M200320001          0.000000         0.000000          0.000000   \n",
      "M200320002          0.000000         0.000000          0.000000   \n",
      "M200320003          0.000000         0.000000          0.000000   \n",
      "M200320004          0.000000         0.000000          0.000000   \n",
      "M200401000A         0.000000         0.000000          0.000000   \n",
      "M200401068          0.000000         0.000000          0.000000   \n",
      "M200401069          0.000000         0.000000          0.000000   \n",
      "M200401070          0.000000         0.000000          0.000000   \n",
      "M200401071          0.000000         0.000000          0.000000   \n",
      "M200403000A         0.000000         0.000000          0.000000   \n",
      "M200403005          0.000000         0.000000      12339.778320   \n",
      "M200403006          0.000000         0.000000      13099.562174   \n",
      "M200403007          0.000000      6068.623861          0.000000   \n",
      "M200403008          0.000000         0.000000      13968.877930   \n",
      "M200408000A         0.000000         0.000000      11460.940430   \n",
      "M200408025      13347.687826         0.000000          0.000000   \n",
      "M200408026          0.000000         0.000000          0.000000   \n",
      "M200408027          0.000000         0.000000          0.000000   \n",
      "M200408028          0.000000         0.000000          0.000000   \n",
      "M200414000A         0.000000         0.000000          0.000000   \n",
      "M200414005          0.000000         0.000000          0.000000   \n",
      "M200414006          0.000000         0.000000          0.000000   \n",
      "M200414007          0.000000         0.000000          0.000000   \n",
      "M200414008          0.000000         0.000000          0.000000   \n",
      "M200422000A         0.000000         0.000000          0.000000   \n",
      "M200422146          0.000000         0.000000          0.000000   \n",
      "M200422147          0.000000         0.000000          0.000000   \n",
      "M200422148          0.000000         0.000000          0.000000   \n",
      "M200422149          0.000000         0.000000          0.000000   \n",
      "M200429000A         0.000000         0.000000          0.000000   \n",
      "M200429096          0.000000         0.000000          0.000000   \n",
      "M200429097          0.000000         0.000000          0.000000   \n",
      "M200429098          0.000000         0.000000          0.000000   \n",
      "M200429099          0.000000         0.000000          0.000000   \n",
      "\n",
      "             M102_R1941_14709  M102_R460_14971  M102_R83_14772  \\\n",
      "M200317000A           0.00000         0.000000        0.000000   \n",
      "M200317007            0.00000     12466.348958        0.000000   \n",
      "M200317008            0.00000     15344.393229        0.000000   \n",
      "M200317009            0.00000     11551.824544        0.000000   \n",
      "M200317010            0.00000     11607.303711        0.000000   \n",
      "M200318000A           0.00000         0.000000        0.000000   \n",
      "M200318001            0.00000     12532.278646        0.000000   \n",
      "M200318002            0.00000     12506.890625        0.000000   \n",
      "M200318003            0.00000     12240.861003        0.000000   \n",
      "M200318004            0.00000     15016.190755        0.000000   \n",
      "M200320000A           0.00000         0.000000     6042.141602   \n",
      "M200320001            0.00000     18806.437500        0.000000   \n",
      "M200320002            0.00000         0.000000        0.000000   \n",
      "M200320003            0.00000     19500.960938        0.000000   \n",
      "M200320004            0.00000     18593.492188        0.000000   \n",
      "M200401000A           0.00000         0.000000        0.000000   \n",
      "M200401068            0.00000     16933.933594        0.000000   \n",
      "M200401069            0.00000     17205.248698        0.000000   \n",
      "M200401070            0.00000     17437.691406        0.000000   \n",
      "M200401071            0.00000     16841.357096        0.000000   \n",
      "M200403000A           0.00000         0.000000        0.000000   \n",
      "M200403005            0.00000     17666.860677        0.000000   \n",
      "M200403006            0.00000     26199.192057        0.000000   \n",
      "M200403007            0.00000     16116.474935        0.000000   \n",
      "M200403008            0.00000     19793.592448        0.000000   \n",
      "M200408000A           0.00000         0.000000        0.000000   \n",
      "M200408025            0.00000         0.000000        0.000000   \n",
      "M200408026        14186.92041         0.000000        0.000000   \n",
      "M200408027            0.00000         0.000000        0.000000   \n",
      "M200408028            0.00000         0.000000        0.000000   \n",
      "M200414000A           0.00000         0.000000        0.000000   \n",
      "M200414005            0.00000         0.000000        0.000000   \n",
      "M200414006            0.00000         0.000000        0.000000   \n",
      "M200414007            0.00000         0.000000        0.000000   \n",
      "M200414008            0.00000         0.000000        0.000000   \n",
      "M200422000A           0.00000         0.000000        0.000000   \n",
      "M200422146            0.00000      6062.398926        0.000000   \n",
      "M200422147            0.00000      6125.951823        0.000000   \n",
      "M200422148            0.00000         0.000000        0.000000   \n",
      "M200422149            0.00000         0.000000        0.000000   \n",
      "M200429000A           0.00000         0.000000        0.000000   \n",
      "M200429096            0.00000         0.000000        0.000000   \n",
      "M200429097            0.00000         0.000000        0.000000   \n",
      "M200429098            0.00000         0.000000        0.000000   \n",
      "M200429099            0.00000         0.000000        0.000000   \n",
      "\n",
      "             M102_R85_14668  ...  M98_R98_11545  M99_R1075_13094  \\\n",
      "M200317000A        0.000000  ...       0.000000         0.000000   \n",
      "M200317007     96782.518229  ...       0.000000      5643.669922   \n",
      "M200317008     91067.981771  ...       0.000000         0.000000   \n",
      "M200317009     82281.312500  ...       0.000000      5976.286784   \n",
      "M200317010     47015.386719  ...       0.000000         0.000000   \n",
      "M200318000A        0.000000  ...       0.000000         0.000000   \n",
      "M200318001     91147.309896  ...       0.000000      7023.159342   \n",
      "M200318002     89001.791667  ...       0.000000      5899.195150   \n",
      "M200318003     77890.739583  ...       0.000000      6661.251465   \n",
      "M200318004      8737.904948  ...       0.000000         0.000000   \n",
      "M200320000A        0.000000  ...       0.000000         0.000000   \n",
      "M200320001         0.000000  ...       0.000000      7287.693522   \n",
      "M200320002         0.000000  ...       0.000000      7370.192057   \n",
      "M200320003         0.000000  ...       0.000000      7160.258626   \n",
      "M200320004         0.000000  ...       0.000000         0.000000   \n",
      "M200401000A        0.000000  ...       0.000000         0.000000   \n",
      "M200401068         0.000000  ...       0.000000      7311.761230   \n",
      "M200401069         0.000000  ...       0.000000      6348.134928   \n",
      "M200401070         0.000000  ...       0.000000      7882.436849   \n",
      "M200401071         0.000000  ...       0.000000         0.000000   \n",
      "M200403000A        0.000000  ...       0.000000         0.000000   \n",
      "M200403005         0.000000  ...       0.000000      7196.422201   \n",
      "M200403006         0.000000  ...       0.000000      7071.941569   \n",
      "M200403007         0.000000  ...       0.000000      7308.212402   \n",
      "M200403008         0.000000  ...       0.000000         0.000000   \n",
      "M200408000A        0.000000  ...       0.000000         0.000000   \n",
      "M200408025         0.000000  ...       0.000000      7011.042969   \n",
      "M200408026         0.000000  ...       0.000000      7692.709961   \n",
      "M200408027         0.000000  ...       0.000000      6441.416178   \n",
      "M200408028         0.000000  ...       0.000000         0.000000   \n",
      "M200414000A        0.000000  ...       0.000000         0.000000   \n",
      "M200414005         0.000000  ...       0.000000         0.000000   \n",
      "M200414006         0.000000  ...       0.000000      5974.652018   \n",
      "M200414007         0.000000  ...       0.000000         0.000000   \n",
      "M200414008         0.000000  ...       0.000000         0.000000   \n",
      "M200422000A        0.000000  ...       0.000000         0.000000   \n",
      "M200422146         0.000000  ...       0.000000         0.000000   \n",
      "M200422147         0.000000  ...       0.000000      7197.216634   \n",
      "M200422148         0.000000  ...       0.000000      7230.584147   \n",
      "M200422149         0.000000  ...       0.000000         0.000000   \n",
      "M200429000A        0.000000  ...       0.000000         0.000000   \n",
      "M200429096         0.000000  ...       0.000000      8405.761230   \n",
      "M200429097         0.000000  ...       0.000000      6676.306315   \n",
      "M200429098         0.000000  ...    5869.522461      8023.699870   \n",
      "M200429099         0.000000  ...       0.000000         0.000000   \n",
      "\n",
      "             M99_R1238_13084  M99_R48_12384  M99_R705_13068  M99_R772_13135  \\\n",
      "M200317000A         0.000000        0.00000        0.000000        0.000000   \n",
      "M200317007          0.000000        0.00000        0.000000        0.000000   \n",
      "M200317008          0.000000        0.00000        0.000000        0.000000   \n",
      "M200317009          0.000000        0.00000        0.000000        0.000000   \n",
      "M200317010          0.000000        0.00000        0.000000        0.000000   \n",
      "M200318000A         0.000000        0.00000        0.000000        0.000000   \n",
      "M200318001          0.000000        0.00000        0.000000     5287.698405   \n",
      "M200318002          0.000000        0.00000        0.000000        0.000000   \n",
      "M200318003          0.000000        0.00000        0.000000        0.000000   \n",
      "M200318004          0.000000        0.00000        0.000000        0.000000   \n",
      "M200320000A         0.000000        0.00000        0.000000        0.000000   \n",
      "M200320001      10114.701823        0.00000    21108.156250        0.000000   \n",
      "M200320002      11899.621094        0.00000    20385.216146        0.000000   \n",
      "M200320003          0.000000        0.00000    19060.527344        0.000000   \n",
      "M200320004          0.000000        0.00000        0.000000        0.000000   \n",
      "M200401000A         0.000000        0.00000        0.000000        0.000000   \n",
      "M200401068          0.000000        0.00000    25044.288411        0.000000   \n",
      "M200401069      11760.778971        0.00000    24803.128906        0.000000   \n",
      "M200401070       9159.726562        0.00000    23000.063802        0.000000   \n",
      "M200401071          0.000000        0.00000        0.000000        0.000000   \n",
      "M200403000A         0.000000        0.00000        0.000000        0.000000   \n",
      "M200403005      12666.730469        0.00000        0.000000        0.000000   \n",
      "M200403006      13333.944010        0.00000        0.000000        0.000000   \n",
      "M200403007          0.000000        0.00000        0.000000        0.000000   \n",
      "M200403008          0.000000        0.00000        0.000000        0.000000   \n",
      "M200408000A         0.000000        0.00000        0.000000        0.000000   \n",
      "M200408025       7094.704427        0.00000        0.000000        0.000000   \n",
      "M200408026          0.000000        0.00000        0.000000        0.000000   \n",
      "M200408027          0.000000        0.00000        0.000000        0.000000   \n",
      "M200408028          0.000000        0.00000        0.000000        0.000000   \n",
      "M200414000A         0.000000        0.00000        0.000000        0.000000   \n",
      "M200414005          0.000000        0.00000        0.000000        0.000000   \n",
      "M200414006          0.000000        0.00000        0.000000        0.000000   \n",
      "M200414007          0.000000        0.00000        0.000000        0.000000   \n",
      "M200414008          0.000000        0.00000        0.000000        0.000000   \n",
      "M200422000A         0.000000        0.00000        0.000000        0.000000   \n",
      "M200422146          0.000000        0.00000    28521.664062        0.000000   \n",
      "M200422147          0.000000        0.00000    27284.031250        0.000000   \n",
      "M200422148          0.000000        0.00000    26983.003255        0.000000   \n",
      "M200422149          0.000000        0.00000        0.000000        0.000000   \n",
      "M200429000A         0.000000    71020.36849        0.000000        0.000000   \n",
      "M200429096          0.000000        0.00000    41258.080729        0.000000   \n",
      "M200429097          0.000000        0.00000    41986.101562        0.000000   \n",
      "M200429098          0.000000        0.00000    39427.808594        0.000000   \n",
      "M200429099          0.000000        0.00000        0.000000        0.000000   \n",
      "\n",
      "             M99_R97_12789  M99_R97_13147  M993_R1888_113866  \\\n",
      "M200317000A       0.000000       0.000000           0.000000   \n",
      "M200317007        0.000000       0.000000           0.000000   \n",
      "M200317008        0.000000       0.000000           0.000000   \n",
      "M200317009     5799.598470       0.000000           0.000000   \n",
      "M200317010        0.000000       0.000000           0.000000   \n",
      "M200318000A       0.000000       0.000000           0.000000   \n",
      "M200318001        0.000000       0.000000           0.000000   \n",
      "M200318002        0.000000       0.000000           0.000000   \n",
      "M200318003        0.000000       0.000000           0.000000   \n",
      "M200318004        0.000000       0.000000           0.000000   \n",
      "M200320000A       0.000000       0.000000           0.000000   \n",
      "M200320001     6792.310710       0.000000           0.000000   \n",
      "M200320002     6705.794271       0.000000           0.000000   \n",
      "M200320003     6367.876790       0.000000           0.000000   \n",
      "M200320004        0.000000       0.000000           0.000000   \n",
      "M200401000A       0.000000       0.000000           0.000000   \n",
      "M200401068        0.000000       0.000000           0.000000   \n",
      "M200401069        0.000000       0.000000           0.000000   \n",
      "M200401070     6976.566406    7663.003255           0.000000   \n",
      "M200401071        0.000000       0.000000           0.000000   \n",
      "M200403000A       0.000000       0.000000           0.000000   \n",
      "M200403005     7008.709961       0.000000           0.000000   \n",
      "M200403006     7646.399577       0.000000           0.000000   \n",
      "M200403007     8002.375651       0.000000           0.000000   \n",
      "M200403008        0.000000       0.000000      376593.104167   \n",
      "M200408000A       0.000000       0.000000           0.000000   \n",
      "M200408025        0.000000       0.000000           0.000000   \n",
      "M200408026        0.000000       0.000000           0.000000   \n",
      "M200408027        0.000000       0.000000           0.000000   \n",
      "M200408028        0.000000       0.000000           0.000000   \n",
      "M200414000A       0.000000       0.000000           0.000000   \n",
      "M200414005        0.000000       0.000000           0.000000   \n",
      "M200414006        0.000000       0.000000           0.000000   \n",
      "M200414007        0.000000    7785.830241           0.000000   \n",
      "M200414008        0.000000       0.000000           0.000000   \n",
      "M200422000A       0.000000       0.000000           0.000000   \n",
      "M200422146        0.000000       0.000000           0.000000   \n",
      "M200422147        0.000000       0.000000           0.000000   \n",
      "M200422148        0.000000    8125.634277           0.000000   \n",
      "M200422149        0.000000       0.000000           0.000000   \n",
      "M200429000A       0.000000       0.000000           0.000000   \n",
      "M200429096        0.000000       0.000000           0.000000   \n",
      "M200429097        0.000000       0.000000           0.000000   \n",
      "M200429098        0.000000       0.000000           0.000000   \n",
      "M200429099        0.000000       0.000000           0.000000   \n",
      "\n",
      "             M993_R1893_113846  \n",
      "M200317000A           0.000000  \n",
      "M200317007            0.000000  \n",
      "M200317008            0.000000  \n",
      "M200317009            0.000000  \n",
      "M200317010            0.000000  \n",
      "M200318000A           0.000000  \n",
      "M200318001            0.000000  \n",
      "M200318002            0.000000  \n",
      "M200318003            0.000000  \n",
      "M200318004            0.000000  \n",
      "M200320000A           0.000000  \n",
      "M200320001            0.000000  \n",
      "M200320002            0.000000  \n",
      "M200320003            0.000000  \n",
      "M200320004            0.000000  \n",
      "M200401000A           0.000000  \n",
      "M200401068        19759.971354  \n",
      "M200401069            0.000000  \n",
      "M200401070            0.000000  \n",
      "M200401071            0.000000  \n",
      "M200403000A           0.000000  \n",
      "M200403005            0.000000  \n",
      "M200403006            0.000000  \n",
      "M200403007            0.000000  \n",
      "M200403008            0.000000  \n",
      "M200408000A           0.000000  \n",
      "M200408025            0.000000  \n",
      "M200408026            0.000000  \n",
      "M200408027            0.000000  \n",
      "M200408028            0.000000  \n",
      "M200414000A           0.000000  \n",
      "M200414005            0.000000  \n",
      "M200414006            0.000000  \n",
      "M200414007            0.000000  \n",
      "M200414008            0.000000  \n",
      "M200422000A           0.000000  \n",
      "M200422146            0.000000  \n",
      "M200422147            0.000000  \n",
      "M200422148            0.000000  \n",
      "M200422149            0.000000  \n",
      "M200429000A           0.000000  \n",
      "M200429096            0.000000  \n",
      "M200429097            0.000000  \n",
      "M200429098            0.000000  \n",
      "M200429099            0.000000  \n",
      "\n",
      "[45 rows x 4444 columns]\n"
     ]
    }
   ],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from sklearn.decomposition import PCA \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.print()\n",
    "\n",
    "print(\"Create a list of analysis object and prints it\" )\n",
    "for analysis in engine._analyses:\n",
    "    print(f\"Analysis: {analysis.name}\")\n",
    "    for key, value in analysis.data.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "rownames = engine.get_analyses_names()\n",
    "print(\"Analysename: \", rownames)\n",
    "\n",
    "mat = engine.get_data()\n",
    "mat.index = rownames\n",
    "print(\"Matrix: \\n\", mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Principle Conponent Analysis (PCA)\n",
    "\n",
    "The method implements a machine learning engine that perfporms PCA on the dataset and visualizes the results. ProcessingSetting is the parent of MakePCA. The ProcessingSettings used to assemble data processing workflows within the each engine. The subclass MakePCASKL of MakePCA using skitklearn algorithm to perform the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of the CSV file: {'number_of_rows': 45, 'number_of_columns': 4445}\n",
      "Structure of the CSV file: {'number_of_rows': 45, 'number_of_columns': 2}\n",
      "\n",
      "MachineLearningEngine \n",
      "  name: None \n",
      "  author: None \n",
      "  path: None \n",
      "  date: 2024-08-15 10:35:50.207618 \n",
      "  analyses: 45 \n",
      "  settings: 0 \n",
      "\n",
      "\n",
      "MachineLearningEngine \n",
      "  name: None \n",
      "  author: None \n",
      "  path: None \n",
      "  date: 2024-08-15 10:35:50.207618 \n",
      "  analyses: 45 \n",
      "  settings: 1 \n",
      "\n",
      "Running workflow with settings: MakeModel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import  MakeModelPCASKL\n",
    "import webbrowser\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'new_metadata.csv'\n",
    "engine.add_classes_from_csv(class_path)\n",
    "\n",
    "#engine.print()\n",
    "#print(engine.get_classes())\n",
    "\n",
    "# !!! make a general data plot\n",
    "engine.plot_data()\n",
    "webbrowser.open('general_data_plot.html')\n",
    "# x axis in the index of the features (i.e., col names)\n",
    "# y axis is the valule for each analysis\n",
    "# color legend is applied for each analysis\n",
    "\n",
    "# Add the ProcessingSettings to the _settings attribute with add settings\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "#engine.print()\n",
    "# Create a method in the ML engine to perfom PCA and collect the results\n",
    "engine.run_workflow()\n",
    "# The results are added to the _results atribute of the engine\n",
    "# make a plot method in the ML engine for the PCA results and classes\n",
    "engine.plot_pca()\n",
    "# make a loadings plot after confirming the scores plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelPCASKL\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "class_path = 'new_metadata.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot both month\n",
    "engine.add_month_classes(df, ['march', 'april']) \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "webbrowser.open('pca_scores_plot.html')\n",
    "webbrowser.open('pca_loadings_plot.html')\n",
    "\n",
    "#plot only march\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.add_month_classes(df, 'march') \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "\n",
    "#plot only april\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.add_month_classes(df, 'april') \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Density-Based Spatial Clustering of Application with Noise (DBSCAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.express as px\n",
    "\n",
    "eps = 1.5  \n",
    "min_samples = 5  \n",
    "\n",
    "path = 'feature_list.csv'  \n",
    "engine = MachineLearningEngine()  \n",
    "engine.add_analyses_from_csv(path)  \n",
    "engine.print()  \n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name'])  \n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(data)  \n",
    "\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  \n",
    "n_noise = list(labels).count(-1) \n",
    "print(f\"Anzahl der gesch√§tzten Cluster: {n_clusters}\")\n",
    "print(f\"Anzahl der Rauschpunkte: {n_noise}\")\n",
    "\n",
    "data['Cluster'] = labels  \n",
    "data['Cluster'] = data['Cluster'].astype(str)  \n",
    "\n",
    "random_columns = np.random.choice(data.columns, 2, replace=False)\n",
    "fig = px.scatter(data, x=random_columns[0], y=random_columns[1], color='Cluster',\n",
    "                 title=\"DBSCAN Clustering Results\",\n",
    "                 color_continuous_scale=px.colors.diverging.Tealrose,\n",
    "                 labels={'color': 'Cluster ID'})\n",
    "\n",
    "fig.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Manifold Approximation and Projection (UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import  MakeModelUMAP\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'feature_metadata.csv'\n",
    "engine.add_classes_from_csv(class_path)\n",
    "#engine.print()\n",
    "\n",
    "#plot all classes\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2,random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "#engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()\n",
    "\n",
    "#plot both months\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.add_month_classes(df, ['march', 'april']) \n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2,random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "#engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import  MakeModelUMAP\n",
    "import pandas as pd\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'new_metadata.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "#plot march\n",
    "engine.add_month_classes(df, 'march') \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  \n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot april\n",
    "engine.add_month_classes(df, 'april')  \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "update . . . 1.short data 2.long data (only used the classes csv, idk if is it right!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('new_metadata.csv')\n",
    "target_column = 'class'\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "# used label encoder for encode the columns\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':  \n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le  \n",
    "\n",
    "# split data\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the parameter for gridsearchcv\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 500],\n",
    "    'max_depth': [3, 20]\n",
    "}\n",
    "\n",
    "# initalize random forest classifier and gridsearchcv\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# execute grid\n",
    "print(\"Starte GridSearch...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# see the best hyperparameters and the model for it\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation-Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# final model with the best hyperparameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# evaltuation of accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"ClassificationReport:\\n\", class_report)\n",
    "\n",
    "# confusionmatrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                              index=label_encoders[target_column].classes_, \n",
    "                              columns=label_encoders[target_column].classes_)\n",
    "\n",
    "# visualize the confusionsmatrix\n",
    "fig_conf_matrix = px.imshow(conf_matrix_df, \n",
    "                            labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"), \n",
    "                            x=label_encoders[target_column].classes_, \n",
    "                            y=label_encoders[target_column].classes_,\n",
    "                            title=\"Confusion Matrix\")\n",
    "\n",
    "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
    "fig_conf_matrix.show()\n",
    "\n",
    "# visualize feature importance\n",
    "feature_importances = best_rf.feature_importances_\n",
    "features = X.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "fig = px.bar(importance_df, x='Feature', y='Importance', title='Random Forest Feature Importances')\n",
    "fig.show()\n",
    "\n",
    "# export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('groups_classes.csv')\n",
    "target_column = 'class'\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "# used label encoder for encode the columns\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':  \n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le  \n",
    "\n",
    "# split data\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the parameter for gridsearchcv\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 500],\n",
    "    'max_depth': [3, 20]\n",
    "}\n",
    "\n",
    "# initalize random forest classifier and gridsearchcv\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# execute grid\n",
    "print(\"Starte GridSearch...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# see the best hyperparameters and the model for it\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation-Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# final model with the best hyperparameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# evaltuation of accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"ClassificationReport:\\n\", class_report)\n",
    "\n",
    "# confusionmatrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                              index=label_encoders[target_column].classes_, \n",
    "                              columns=label_encoders[target_column].classes_)\n",
    "\n",
    "# visualize the confusionsmatrix\n",
    "fig_conf_matrix = px.imshow(conf_matrix_df, \n",
    "                            labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"), \n",
    "                            x=label_encoders[target_column].classes_, \n",
    "                            y=label_encoders[target_column].classes_,\n",
    "                            title=\"Confusion Matrix\")\n",
    "\n",
    "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
    "fig_conf_matrix.show()\n",
    "\n",
    "# visualize feature importance\n",
    "feature_importances = best_rf.feature_importances_\n",
    "features = X.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "fig = px.bar(importance_df, x='Feature', y='Importance', title='Random Forest Feature Importances')\n",
    "fig.show()\n",
    "\n",
    "# export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Density-Based Spatial Clustering of Application (HDBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.print()\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name'])\n",
    "\n",
    "label_encoders = {}\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':  \n",
    "        le = LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "\n",
    "random_columns = np.random.choice(data.columns, 2, replace=False)\n",
    "\n",
    "clusterer = HDBSCAN(min_cluster_size=3)\n",
    "cluster_labels = clusterer.fit_predict(data)\n",
    "data['cluster'] = cluster_labels  \n",
    "\n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    x=random_columns[0],\n",
    "    y=random_columns[1],\n",
    "    color='cluster',\n",
    "    title='HDBSCAN Clustering mit Plotly'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name']) \n",
    "\n",
    "# Define number of clusters for KMeans\n",
    "n_clusters = 4  # Specify the number of clusters you expect\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "labels = kmeans.fit_predict(data)\n",
    "\n",
    "data['Cluster'] = labels.astype(str) \n",
    "\n",
    "random_columns = np.random.choice(data.columns[:-1], 2, replace=False)\n",
    "\n",
    "fig = px.scatter(\n",
    "    data, \n",
    "    x=random_columns[0], \n",
    "    y=random_columns[1], \n",
    "    color='Cluster',\n",
    "    title=\"KMeans Clustering Results\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    "    labels={'color': 'Cluster ID'}\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW DATA ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### umap plot for the 'negative' and 'positive' classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot negative polarity classes\n",
    "engine.add_polarity_classes(df, 'negative') \n",
    "\n",
    "umap_model_neg = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model_neg)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()\n",
    "\n",
    "#plot positive polarity classes\n",
    "engine = MachineLearningEngine()  \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_polarity_classes(df, 'positive')\n",
    "\n",
    "umap_model_pos = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model_pos)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot both polarity\n",
    "engine.add_polarity_classes(df, ['negative', 'positive']) \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca plot for the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import  MakeModelPCASKL\n",
    "import webbrowser\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'groups_ints.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'groups_classes.csv'\n",
    "engine.add_classes_from_csv(class_path)\n",
    "engine.print()\n",
    "engine.plot_data()\n",
    "# webbrowser.open('general_data_plot.html')\n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "#engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "# webbrowser.open('pca_scores_plot.html')\n",
    "# webbrowser.open('pca_loadings_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot pca for the 'negative' and 'positive' classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import  MakeModelPCASKL\n",
    "import pandas as pd\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "# Plot negative polarity classes in PCA\n",
    "engine.add_polarity_classes(df, 'negative')\n",
    "\n",
    "pca_model_neg = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model_neg)\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot positive polarity classes in PCA\n",
    "engine.add_polarity_classes(df, 'positive')\n",
    "\n",
    "pca_model_pos = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model_pos)\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbscan plot for the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'groups_classes.csv'\n",
    "engine.add_classes_from_csv(class_path)\n",
    "\n",
    "# Set DBSCAN parameters\n",
    "eps = 1.5  # Radius around a point to consider others as neighbors\n",
    "min_samples = 5  # Minimum neighbors a point must have to not be considered noise\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name']) \n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(data) \n",
    "\n",
    "# Calculate and print estimated number of clusters and noise points\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Number of clusters\n",
    "n_noise = list(labels).count(-1)  # Number of noise points\n",
    "print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "\n",
    "# Add cluster labels to the data\n",
    "data['Cluster'] = labels\n",
    "data['Cluster'] = data['Cluster'].astype(str)  # Convert cluster labels to strings for categorical display\n",
    "\n",
    "random_columns = np.random.choice(data.columns, 2, replace=False)\n",
    "\n",
    "fig = px.scatter(\n",
    "    data, \n",
    "    x=random_columns[0], \n",
    "    y=random_columns[1], \n",
    "    color='Cluster',\n",
    "    title=\"DBSCAN Clustering Results\",\n",
    "    color_continuous_scale=px.colors.diverging.Tealrose,\n",
    "    labels={'color': 'Cluster ID'}\n",
    ")\n",
    "fig.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest plot\n",
    "\n",
    "show you at the meeting. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import Image\n",
    "from scipy.stats import randint\n",
    "\n",
    "# read data\n",
    "features_df = pd.read_csv('groups_ints.csv')  \n",
    "labels_df = pd.read_csv('groups_classes.csv') \n",
    "merged_df = pd.merge(features_df, labels_df, on='name')\n",
    "print(merged_df.info())\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "\n",
    "# prepare data\n",
    "X = merged_df.drop(['name','class'], axis=1)\n",
    "y = merged_df['class']\n",
    "\n",
    "# use oridalencoder\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_encoded = ordinal_encoder.fit_transform(X)\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 500],\n",
    "    'max_depth': [3, 20]\n",
    "    }\n",
    "\n",
    "# initialisiere and fit randomforestclassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train) # train the model\n",
    "y_pred_oe = rf.predict(X_test)\n",
    "\n",
    "# use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(estimator=rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# fit the random search object to the data\n",
    "print(\"Starting RandomizedSearch...\")\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)\n",
    "\n",
    "# create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# add accuracy\n",
    "accuracy_oe = accuracy_score(y_test, y_pred_oe)\n",
    "print(\"Accuracy: \", accuracy_oe)\n",
    "#print(\"Classification report:\\n\", classification_report(y_test, y_pred_oe))\n",
    "\n",
    "# create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_oe)\n",
    "cm_df = pd.DataFrame(cm, index=best_rf.classes_, columns=best_rf.classes_)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "\n",
    "# plot the confusion matrix using plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=cm_df.values,\n",
    "                    x=cm_df.columns,\n",
    "                    y=cm_df.index,\n",
    "                    colorscale='Blues',\n",
    "                    zmin=0, zmax=np.max(cm_df.values),\n",
    "                    colorbar=dict(title='Count'),\n",
    "                    hovertemplate='True Label: %{y}<br>Predicted: %{x}<br>Count: %{z}'\n",
    "                ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='True Label',\n",
    "    autosize=True,\n",
    "    xaxis=dict(tickmode='array', tickvals=np.arange(len(cm_df.columns)), ticktext=cm_df.columns),\n",
    "    yaxis=dict(tickmode='array', tickvals=np.arange(len(cm_df.index)), ticktext=cm_df.index)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = Source(dot_data)\n",
    "    display(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy the encoded x_train and x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from scipy.stats import randint\n",
    "\n",
    "# read data\n",
    "features_df = pd.read_csv('groups_ints.csv')  \n",
    "labels_df = pd.read_csv('groups_classes.csv') \n",
    "merged_df = pd.merge(features_df, labels_df, on='name')\n",
    "print(merged_df.info())\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "\n",
    "# prepare data\n",
    "X = merged_df.drop(['name', 'class'], axis=1)\n",
    "y = merged_df['class']\n",
    "\n",
    "# column to encode\n",
    "cols_to_encode = ['polarity', 'month', 'year','tag','monthclass']\n",
    "non_numerical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# use ordinalencoder\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "X_train[cols_to_encode] = ordinal_encoder.fit_transform(X_train[cols_to_encode])\n",
    "X_test[cols_to_encode] = ordinal_encoder.transform(X_test[cols_to_encode])\n",
    "\n",
    "# initialisiere and fit randomforestclassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred_oe = rf_classifier.predict(X_test)\n",
    "\n",
    "# use randomizedsearchcv\n",
    "param_dist = {'n_estimators': randint(50, 500), 'max_depth': randint(1, 20)}\n",
    "rand_search = RandomizedSearchCV(\n",
    "    rf_classifier, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=5, \n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "rand_search.fit(X_train, y_train)\n",
    "best_rf = rand_search.best_estimator_\n",
    "\n",
    "# see best hyperparameter and accuracy\n",
    "print(\"Best hyperparameters: \", rand_search.best_params_)\n",
    "accuracy_oe = accuracy_score(y_test, y_pred_oe)\n",
    "print(\"Accuracy: \", accuracy_oe)\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred_oe))\n",
    "\n",
    "# confusionsmatrix\n",
    "cm = confusion_matrix(y_test, y_pred_oe)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=best_rf.classes_, columns=best_rf.classes_)\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm_df.values,\n",
    "    x=cm_df.columns,\n",
    "    y=cm_df.index,\n",
    "    colorscale='Blues',\n",
    "    zmin=0, zmax=np.max(cm_df.values),\n",
    "    colorbar=dict(title='Count'),\n",
    "    hovertemplate='True Label: %{y}<br>Predicted: %{x}<br>Count: %{z}'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='True Label',\n",
    "    autosize=True\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = rf_classifier.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = Source(dot_data)\n",
    "    display(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from scipy.stats import randint\n",
    "\n",
    "# read data\n",
    "features_df = pd.read_csv('feature_list.csv')  \n",
    "labels_df = pd.read_csv('new_metadata.csv') \n",
    "merged_df = pd.merge(features_df, labels_df, on='name')\n",
    "print(merged_df.info())\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "\n",
    "# prepare data\n",
    "X = merged_df.drop(['name','class'], axis=1)\n",
    "y = merged_df['class']\n",
    "\n",
    "# use oridalencoder\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_encoded = ordinal_encoder.fit_transform(X)\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 500],\n",
    "    'max_depth': [3, 20]\n",
    "    }\n",
    "\n",
    "# initialisiere and fit RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train) # train the model\n",
    "y_pred_oe = rf.predict(X_test)\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(estimator=rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "# Fit the random search object to the data\n",
    "print(\"Starting RandomizedSearch...\")\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\",  rand_search.best_params_)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# add accuracy\n",
    "accuracy_oe = accuracy_score(y_test, y_pred_oe)\n",
    "print(\"Accuracy: \", accuracy_oe)\n",
    "#print(\"Classification report:\\n\", classification_report(y_test, y_pred_oe))\n",
    "\n",
    "# create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_oe)\n",
    "cm_df = pd.DataFrame(cm, index=best_rf.classes_, columns=best_rf.classes_)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "\n",
    "# plot the confusion matrix using plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=cm_df.values,\n",
    "                    x=cm_df.columns,\n",
    "                    y=cm_df.index,\n",
    "                    colorscale='Blues',\n",
    "                    zmin=0, zmax=np.max(cm_df.values),\n",
    "                    colorbar=dict(title='Count'),\n",
    "                    hovertemplate='True Label: %{y}<br>Predicted: %{x}<br>Count: %{z}'\n",
    "                ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='True Label',\n",
    "    autosize=True,\n",
    "    xaxis=dict(tickmode='array', tickvals=np.arange(len(cm_df.columns)), ticktext=cm_df.columns),\n",
    "    yaxis=dict(tickmode='array', tickvals=np.arange(len(cm_df.index)), ticktext=cm_df.index)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = Source(dot_data)\n",
    "    display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hdbscan plot for the new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.print()\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name'])\n",
    "\n",
    "label_encoders = {}\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object': \n",
    "        le = LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "random_columns = np.random.choice(data.columns, 2, replace=False)\n",
    "\n",
    "clusterer = HDBSCAN(min_cluster_size=3)\n",
    "cluster_labels = clusterer.fit_predict(data)\n",
    "data['cluster'] = cluster_labels  \n",
    "\n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    x=random_columns[0],\n",
    "    y=random_columns[1],\n",
    "    color='cluster',\n",
    "    title='HDBSCAN Clustering mit Plotly'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('groups_ints.csv')\n",
    "data = df.drop(columns=['name'])\n",
    "\n",
    "label_encoders = {}\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object': \n",
    "        le = LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        label_encoders[column] = le  \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42) \n",
    "cluster_labels = kmeans.fit_predict(data)\n",
    "\n",
    "pca_df = pd.DataFrame(data_pca, columns=['PCA1', 'PCA2'])\n",
    "pca_df['Cluster'] = cluster_labels.astype(str)  \n",
    "\n",
    "fig = px.scatter(pca_df, x='PCA1', y='PCA2', color='Cluster', title='KMeans Clustering Results',\n",
    "                 labels={'PCA1': 'Principal Component 1', 'PCA2': 'Principal Component 2'})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name'])  \n",
    "\n",
    "# Define number of clusters for KMeans\n",
    "n_clusters = 10  # Specify the number of clusters you expect\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "labels = kmeans.fit_predict(data)\n",
    "\n",
    "data['Cluster'] = labels.astype(str)  # Convert to string for color coding in plot\n",
    "\n",
    "random_columns = np.random.choice(data.columns[:-1], 2, replace=False)\n",
    "\n",
    "fig = px.scatter(\n",
    "    data, \n",
    "    x=random_columns[0], \n",
    "    y=random_columns[1], \n",
    "    color='Cluster',\n",
    "    title=\"KMeans Clustering Results\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    "    labels={'color': 'Cluster ID'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter 'month' with umap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'may')  \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"plot pos classes\")\n",
    "engine = MachineLearningEngine()  \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'april')  \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'april')  \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine, MachineLearningAnalysis\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        row_value = row.tolist()[1:] \n",
    "        class_name = row['month']\n",
    "        ana = MachineLearningAnalysis(name=str(class_name), \n",
    "                                          data={\"x\": np.array(df.columns.tolist()[1:]), \n",
    "                                                \"y\": np.array(row_value)})\n",
    "        if ana.validate():\n",
    "            engine.add_classes(class_name)\n",
    "        else:\n",
    "            print(f\"Analysis {class_name} did not pass validation.\") \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter 'month' plot with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelPCASKL\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'may') \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "webbrowser.open('pca_scores_plot.html')\n",
    "webbrowser.open('pca_loadings_plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelPCASKL\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'december') \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 10, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "webbrowser.open('pca_scores_plot.html')\n",
    "webbrowser.open('pca_loadings_plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelPCASKL\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, ['may', 'april']) \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "webbrowser.open('pca_scores_plot.html')\n",
    "webbrowser.open('pca_loadings_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca umap test for feautre list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "df_data = pd.read_csv(path)\n",
    "class_path = 'new_metadata.csv'\n",
    "df_classes = pd.read_csv(class_path)\n",
    "\n",
    "print(df_data.shape)\n",
    "print(df_classes.shape)\n",
    "\n",
    "df_merged = pd.merge(df_data, df_classes)\n",
    "print(df_merged.shape)\n",
    "print(df_merged.head())\n",
    "\n",
    "# standardize dataset\n",
    "df_data_numeric = df_merged.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_data_numeric)\n",
    "\n",
    "# the month with the most entries\n",
    "month_counts = df_merged['month'].value_counts()\n",
    "print(\"Anzahl der Eintr√§ge pro Monat:\\n\", month_counts)\n",
    "\n",
    "# unique months/classes\n",
    "unique_months = df_merged['month'].unique()\n",
    "unique_classes = df_merged['class'].unique()\n",
    "\n",
    "# find number of components\n",
    "nums = np.arange(1,20)\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "  pca = PCA(n_components=num)\n",
    "  pca.fit(data_scaled)\n",
    "  var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "df_var = pd.DataFrame({\"num\" : nums, \"var_ratio\" : var_ratio})\n",
    "fig = px.line(df_var, x='num', y='var_ratio', markers=True,\n",
    "              title=\"Explained Variance\",\n",
    "              labels={'num': 'Number of Components', 'var_ratio': 'Explained Variance Ratio'})\n",
    "fig.show()\n",
    "\n",
    "# pca\n",
    "n_components=10\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "print(principal_components.shape)\n",
    "\n",
    "pca_df = pd.DataFrame(data=principal_components[:, [0,1]],columns=['PC1', 'PC2'])\n",
    "pca_df['class'] = df_merged['class'].values  \n",
    "pca_df['month'] = df_merged['month'].values \n",
    "\n",
    "# get min/max for axis scaling\n",
    "offset = 5\n",
    "minC1 = pca_df.min()[0] - offset\n",
    "maxC1 = pca_df.max()[0] + offset\n",
    "minC2 = pca_df.min()[1] - offset\n",
    "maxC2 = pca_df.max()[1] + offset\n",
    "fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2', color='class',\n",
    "        facet_row='month',\n",
    "        title=f'PCA results by month',\n",
    "        labels={'PC1': 'Hauptkomponente 1', 'PC2': 'Hauptkomponente 2', 'color': 'class'},\n",
    "        range_x=[minC1, maxC1], range_y=[minC2, maxC2]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# loop through each class\n",
    "fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2', color='month', symbol='month',\n",
    "        facet_row='class',\n",
    "        title=f'PCA results by class',\n",
    "        labels={'PC1': 'Hauptkomponente 1', 'PC2': 'Hauptkomponente 2', 'color': 'month'},\n",
    "        range_x=[minC1, maxC1], range_y=[minC2, maxC2]\n",
    "    )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "df_data = pd.read_csv(path)\n",
    "class_path = 'new_metadata.csv'\n",
    "df_classes = pd.read_csv(class_path)\n",
    "\n",
    "df_merged = pd.merge(df_data, df_classes)\n",
    "df_data_numeric = df_merged.select_dtypes(include=[np.number])\n",
    "\n",
    "# standardize dataset\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_data_numeric)\n",
    "\n",
    "# use umap \n",
    "n_components = 2\n",
    "umap_model = umap.UMAP(n_components=n_components, random_state=42)\n",
    "umap_components = umap_model.fit_transform(data_scaled)\n",
    "\n",
    "umap_df = pd.DataFrame(data=umap_components, columns=['UMAP1', 'UMAP2'])\n",
    "umap_df['class'] = df_merged['class'].values\n",
    "umap_df['month'] = df_merged['month'].values\n",
    "\n",
    "fig = px.scatter(umap_df, x='UMAP1', y='UMAP2', color='month', title='UMAP f√ºr alle Monate',\n",
    "                 labels={'UMAP1': 'UMAP Komponente 1', 'UMAP2': 'UMAP Komponente 2'})\n",
    "fig.show()\n",
    "\n",
    "# find the month with the max entries\n",
    "month_counts = df_merged['month'].value_counts()\n",
    "max_entries_month = month_counts.idxmax()\n",
    "max_entries_count = month_counts.max()\n",
    "\n",
    "print(f\"Der Monat mit den meisten Eintr√§gen ist {max_entries_month} mit {max_entries_count} Eintr√§gen.\")\n",
    "print(\"Anzahl der Eintr√§ge pro Monat:\\n\", month_counts)\n",
    "\n",
    "\n",
    "# unique months\n",
    "unique_months = df_merged['month'].unique()\n",
    "\n",
    "# loop through each month\n",
    "for month in unique_months:\n",
    "    # filter data for the current month\n",
    "    month_data = umap_df[umap_df['month'] == month]\n",
    "    \n",
    "    # plot pca for current month\n",
    "    fig = px.scatter(\n",
    "        month_data, x='UMAP1', y='UMAP2', color='class',\n",
    "        title=f'UMAP 2D Scatter Plot f√ºr {month}',\n",
    "        labels={'color': 'class'}\n",
    "    )\n",
    "    fig.update_layout(xaxis_title=\"UMAP1\", yaxis_title=\"UMAP2\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test umap and pca for groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingSettings import MakeModelUMAP\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "df_data = pd.read_csv(path)\n",
    "class_path = 'groups_classes.csv'\n",
    "df_classes = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.add_classes_from_csv(class_path)\n",
    "engine.print()\n",
    "\n",
    "df_merged = pd.merge(df_data, df_classes)\n",
    "df_data_numeric = df_merged.select_dtypes(include=[np.number])\n",
    "\n",
    "# standardize dataset\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_data_numeric)\n",
    "\n",
    "# use umap \n",
    "n_components = 2\n",
    "umap_model = umap.UMAP(n_components=n_components, random_state=42)\n",
    "umap_components = umap_model.fit_transform(data_scaled)\n",
    "\n",
    "umap_df = pd.DataFrame(data=umap_components, columns=['UMAP1', 'UMAP2'])\n",
    "umap_df['class'] = df_merged['class'].values\n",
    "umap_df['month'] = df_merged['month'].values\n",
    "\n",
    "fig = px.scatter(umap_df, x='UMAP1', y='UMAP2', color='month', title='UMAP f√ºr alle Monate',\n",
    "                 labels={'UMAP1': 'UMAP Komponente 1', 'UMAP2': 'UMAP Komponente 2'})\n",
    "fig.show()\n",
    "\n",
    "# find the month with the max entries\n",
    "month_counts = df_merged['month'].value_counts()\n",
    "max_entries_month = month_counts.idxmax()\n",
    "max_entries_count = month_counts.max()\n",
    "\n",
    "print(f\"Der Monat mit den meisten Eintr√§gen ist {max_entries_month} mit {max_entries_count} Eintr√§gen.\")\n",
    "print(\"Anzahl der Eintr√§ge pro Monat:\\n\", month_counts)\n",
    "\n",
    "\n",
    "# unique months\n",
    "unique_months = df_merged['month'].unique()\n",
    "\n",
    "# loop through each month\n",
    "for month in unique_months:\n",
    "    # filter data for the current month\n",
    "    month_data = umap_df[umap_df['month'] == month]\n",
    "    \n",
    "    # plot pca for current month\n",
    "    fig = px.scatter(\n",
    "        month_data, x='UMAP1', y='UMAP2', color='class',\n",
    "        title=f'UMAP 2D Scatter Plot f√ºr {month}',\n",
    "        labels={'color': 'class'}\n",
    "    )\n",
    "    fig.update_layout(xaxis_title=\"UMAP1\", yaxis_title=\"UMAP2\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "df_data = pd.read_csv(path)\n",
    "class_path = 'groups_classes.csv'\n",
    "df_classes = pd.read_csv(class_path)\n",
    "\n",
    "print(df_data.shape)\n",
    "print(df_classes.shape)\n",
    "\n",
    "df_merged = pd.merge(df_data, df_classes)\n",
    "print(df_merged.shape)\n",
    "print(df_merged.head())\n",
    "\n",
    "# standardize dataset\n",
    "df_data_numeric = df_merged.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_data_numeric)\n",
    "\n",
    "# the month with the most entries\n",
    "month_counts = df_merged['month'].value_counts()\n",
    "print(\"Anzahl der Eintr√§ge pro Monat:\\n\", month_counts)\n",
    "\n",
    "# unique months/classes\n",
    "unique_months = df_merged['month'].unique()\n",
    "unique_classes = df_merged['class'].unique()\n",
    "\n",
    "# find number of components\n",
    "nums = np.arange(1,20)\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "    pca = PCA(n_components=num)\n",
    "    pca.fit(data_scaled)\n",
    "    var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "df_var = pd.DataFrame({\"num\" : nums, \"var_ratio\" : var_ratio})\n",
    "fig = px.line(df_var, x='num', y='var_ratio', markers=True,\n",
    "              title=\"Explained Variance\",\n",
    "              labels={'num': 'Number of Components', 'var_ratio': 'Explained Variance Ratio'})\n",
    "fig.show()\n",
    "\n",
    "# pca\n",
    "n_components=10\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "print(principal_components.shape)\n",
    "\n",
    "pca_df = pd.DataFrame(data=principal_components[:, [0,1]],columns=['PC1', 'PC2'])\n",
    "pca_df['class'] = df_merged['class'].values  \n",
    "pca_df['month'] = df_merged['month'].values \n",
    "\n",
    "# get min/max for axis scaling\n",
    "offset = 5\n",
    "minC1 = pca_df.min()[0] - offset\n",
    "maxC1 = pca_df.max()[0] + offset\n",
    "minC2 = pca_df.min()[1] - offset\n",
    "maxC2 = pca_df.max()[1] + offset\n",
    "fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2', color='class',\n",
    "        facet_row='month',\n",
    "        title=f'PCA results by month',\n",
    "        labels={'PC1': 'Hauptkomponente 1', 'PC2': 'Hauptkomponente 2', 'color': 'class'},\n",
    "        range_x=[minC1, maxC1], range_y=[minC2, maxC2]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# loop through each class\n",
    "fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2', color='month', symbol='month',\n",
    "        facet_row='class',\n",
    "        title=f'PCA results by class',\n",
    "        labels={'PC1': 'Hauptkomponente 1', 'PC2': 'Hauptkomponente 2', 'color': 'month'},\n",
    "        range_x=[minC1, maxC1], range_y=[minC2, maxC2]\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
