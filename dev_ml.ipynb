{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MachineLearningEngine Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MachineLearningEngine class is builds on the Engine class. The Engine class serves as a parent class engines that focus on data, while the MachineLearningEngine class is for engines that focus on learning from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "engine = MachineLearningEngine()\n",
    "engine.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MachineLearningAnalyses Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MachineLearningAnalyses class is builds on the class Analyses. The Analyses class that is used to perform Analyses on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningAnalyses import MachineLearningAnalyses\n",
    "\n",
    "#Creates an empty MachineLearningAnalyses obejct and prints it\n",
    "Analyses = MachineLearningAnalyses()\n",
    "Analyses.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CSV File  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method loads the dataset from csv file and create a list of Analyses object. Used the data to make a matrix with the Analyses names and visualizes the results using a scatter plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from sklearn.decomposition import PCA \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.print()\n",
    "\n",
    "print(\"Create a list of Analyses object and prints it\" )\n",
    "for Analyses in engine._analyses:\n",
    "    print(f\"Analyses: {Analyses.name}\")\n",
    "    for key, value in Analyses.data.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "rownames = engine.get_analyses_names()\n",
    "print(\"Analysename: \", rownames)\n",
    "\n",
    "mat = engine.get_data()\n",
    "mat.index = rownames\n",
    "print(\"Matrix: \\n\", mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Principle Conponent Analyses (PCA)\n",
    "\n",
    "The method implements a machine learning engine that perfporms PCA on the dataset and visualizes the results. ProcessingSetting is the parent of MakePCA. The ProcessingMethod used to assemble data processing workflows within the each engine. The subclass MakePCASKL of MakePCA using skitklearn algorithm to perform the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import  MakeModelPCASKL\n",
    "import webbrowser\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'new_metadata.csv'\n",
    "engine.add_classes_from_csv(class_path)\n",
    "\n",
    "#engine.print()\n",
    "#print(engine.get_classes())\n",
    "\n",
    "# !!! make a general data plot\n",
    "engine.plot_data()\n",
    "webbrowser.open('general_data_plot.html')\n",
    "# x axis in the index of the features (i.e., col names)\n",
    "# y axis is the valule for each Analyses\n",
    "# color legend is applied for each Analyses\n",
    "\n",
    "# Add the ProcessingMethod to the _settings attribute with add settings\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "#engine.print()\n",
    "# Create a method in the ML engine to perfom PCA and collect the results\n",
    "engine.run_workflow()\n",
    "# The results are added to the _results atribute of the engine\n",
    "# make a plot method in the ML engine for the PCA results and classes\n",
    "engine.plot_pca()\n",
    "# make a loadings plot after confirming the scores plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelPCASKL\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "class_path = 'new_metadata.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot both month\n",
    "engine.add_month_classes(df, ['march', 'april']) \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "webbrowser.open('pca_scores_plot.html')\n",
    "webbrowser.open('pca_loadings_plot.html')\n",
    "\n",
    "#plot only march\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.add_month_classes(df, 'march') \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "\n",
    "#plot only april\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.add_month_classes(df, 'april') \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a Density-Based Spatial Clustering of Application with Noise (DBSCAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.express as px\n",
    "\n",
    "eps = 1.5  \n",
    "min_samples = 5  \n",
    "\n",
    "path = 'feature_list.csv'  \n",
    "engine = MachineLearningEngine()  \n",
    "engine.add_analyses_from_csv(path)  \n",
    "engine.print()  \n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name'])  \n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(data)  \n",
    "\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  \n",
    "n_noise = list(labels).count(-1) \n",
    "print(f\"Anzahl der gesch√§tzten Cluster: {n_clusters}\")\n",
    "print(f\"Anzahl der Rauschpunkte: {n_noise}\")\n",
    "\n",
    "data['Cluster'] = labels  \n",
    "data['Cluster'] = data['Cluster'].astype(str)  \n",
    "\n",
    "random_columns = np.random.choice(data.columns, 2, replace=False)\n",
    "fig = px.scatter(data, x=random_columns[0], y=random_columns[1], color='Cluster',\n",
    "                 title=\"DBSCAN Clustering Results\",\n",
    "                 color_continuous_scale=px.colors.diverging.Tealrose,\n",
    "                 labels={'color': 'Cluster ID'})\n",
    "\n",
    "fig.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Manifold Approximation and Projection (UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import  MakeModelUMAP\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'feature_metadata.csv'\n",
    "engine.add_classes_from_csv(class_path)\n",
    "#engine.print()\n",
    "\n",
    "#plot all classes\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2,random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "#engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()\n",
    "\n",
    "#plot both months\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.add_month_classes(df, ['march', 'april']) \n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2,random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "#engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import  MakeModelUMAP\n",
    "import pandas as pd\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'new_metadata.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "#plot march\n",
    "engine.add_month_classes(df, 'march') \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  \n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot april\n",
    "engine.add_month_classes(df, 'april')  \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "update . . . 1.short data 2.long data (only used the classes csv, idk if is it right!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('new_metadata.csv')\n",
    "target_column = 'class'\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "# used label encoder for encode the columns\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':  \n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le  \n",
    "\n",
    "# split data\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the parameter for gridsearchcv\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 500],\n",
    "    'max_depth': [3, 20]\n",
    "}\n",
    "\n",
    "# initalize random forest classifier and gridsearchcv\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# execute grid\n",
    "print(\"Starte GridSearch...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# see the best hyperparameters and the model for it\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation-Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# final model with the best hyperparameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# evaltuation of accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"ClassificationReport:\\n\", class_report)\n",
    "\n",
    "# confusionmatrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                              index=label_encoders[target_column].classes_, \n",
    "                              columns=label_encoders[target_column].classes_)\n",
    "\n",
    "# visualize the confusionsmatrix\n",
    "fig_conf_matrix = px.imshow(conf_matrix_df, \n",
    "                            labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"), \n",
    "                            x=label_encoders[target_column].classes_, \n",
    "                            y=label_encoders[target_column].classes_,\n",
    "                            title=\"Confusion Matrix\")\n",
    "\n",
    "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
    "fig_conf_matrix.show()\n",
    "\n",
    "# visualize feature importance\n",
    "feature_importances = best_rf.feature_importances_\n",
    "features = X.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "fig = px.bar(importance_df, x='Feature', y='Importance', title='Random Forest Feature Importances')\n",
    "fig.show()\n",
    "\n",
    "# export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('groups_classes.csv')\n",
    "target_column = 'class'\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "# used label encoder for encode the columns\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':  \n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le  \n",
    "\n",
    "# split data\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the parameter for gridsearchcv\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 500],\n",
    "    'max_depth': [3, 20]\n",
    "}\n",
    "\n",
    "# initalize random forest classifier and gridsearchcv\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# execute grid\n",
    "print(\"Starte GridSearch...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# see the best hyperparameters and the model for it\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation-Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# final model with the best hyperparameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# evaltuation of accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"ClassificationReport:\\n\", class_report)\n",
    "\n",
    "# confusionmatrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                              index=label_encoders[target_column].classes_, \n",
    "                              columns=label_encoders[target_column].classes_)\n",
    "\n",
    "# visualize the confusionsmatrix\n",
    "fig_conf_matrix = px.imshow(conf_matrix_df, \n",
    "                            labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"), \n",
    "                            x=label_encoders[target_column].classes_, \n",
    "                            y=label_encoders[target_column].classes_,\n",
    "                            title=\"Confusion Matrix\")\n",
    "\n",
    "fig_conf_matrix.update_layout(coloraxis_showscale=True)\n",
    "fig_conf_matrix.show()\n",
    "\n",
    "# visualize feature importance\n",
    "feature_importances = best_rf.feature_importances_\n",
    "features = X.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "fig = px.bar(importance_df, x='Feature', y='Importance', title='Random Forest Feature Importances')\n",
    "fig.show()\n",
    "\n",
    "# export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Density-Based Spatial Clustering of Application (HDBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.print()\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name'])\n",
    "\n",
    "label_encoders = {}\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':  \n",
    "        le = LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "\n",
    "random_columns = np.random.choice(data.columns, 2, replace=False)\n",
    "\n",
    "clusterer = HDBSCAN(min_cluster_size=3)\n",
    "cluster_labels = clusterer.fit_predict(data)\n",
    "data['cluster'] = cluster_labels  \n",
    "\n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    x=random_columns[0],\n",
    "    y=random_columns[1],\n",
    "    color='cluster',\n",
    "    title='HDBSCAN Clustering mit Plotly'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name']) \n",
    "\n",
    "# Define number of clusters for KMeans\n",
    "n_clusters = 4  # Specify the number of clusters you expect\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "labels = kmeans.fit_predict(data)\n",
    "\n",
    "data['Cluster'] = labels.astype(str) \n",
    "\n",
    "random_columns = np.random.choice(data.columns[:-1], 2, replace=False)\n",
    "\n",
    "fig = px.scatter(\n",
    "    data, \n",
    "    x=random_columns[0], \n",
    "    y=random_columns[1], \n",
    "    color='Cluster',\n",
    "    title=\"KMeans Clustering Results\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    "    labels={'color': 'Cluster ID'}\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW DATA ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### umap plot for the 'negative' and 'positive' classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot negative polarity classes\n",
    "engine.add_polarity_classes(df, 'negative') \n",
    "\n",
    "umap_model_neg = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model_neg)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()\n",
    "\n",
    "#plot positive polarity classes\n",
    "engine = MachineLearningEngine()  \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_polarity_classes(df, 'positive')\n",
    "\n",
    "umap_model_pos = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model_pos)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot both polarity\n",
    "engine.add_polarity_classes(df, ['negative', 'positive']) \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca plot for the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import  MakeModelPCASKL\n",
    "import webbrowser\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'groups_ints.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'groups_classes.csv'\n",
    "engine.add_classes_from_csv(class_path)\n",
    "engine.print()\n",
    "engine.plot_data()\n",
    "# webbrowser.open('general_data_plot.html')\n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "#engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "# webbrowser.open('pca_scores_plot.html')\n",
    "# webbrowser.open('pca_loadings_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot pca for the 'negative' and 'positive' classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import  MakeModelPCASKL\n",
    "import pandas as pd\n",
    "\n",
    "#Creates an empty MachineLearningEngine object and prints it\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "# Plot negative polarity classes in PCA\n",
    "engine.add_polarity_classes(df, 'negative')\n",
    "\n",
    "pca_model_neg = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model_neg)\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "#plot positive polarity classes in PCA\n",
    "engine.add_polarity_classes(df, 'positive')\n",
    "\n",
    "pca_model_pos = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model_pos)\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbscan plot for the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "class_path = 'groups_classes.csv'\n",
    "engine.add_classes_from_csv(class_path)\n",
    "\n",
    "# Set DBSCAN parameters\n",
    "eps = 1.5  # Radius around a point to consider others as neighbors\n",
    "min_samples = 5  # Minimum neighbors a point must have to not be considered noise\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name']) \n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(data) \n",
    "\n",
    "# Calculate and print estimated number of clusters and noise points\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Number of clusters\n",
    "n_noise = list(labels).count(-1)  # Number of noise points\n",
    "print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "\n",
    "# Add cluster labels to the data\n",
    "data['Cluster'] = labels\n",
    "data['Cluster'] = data['Cluster'].astype(str)  # Convert cluster labels to strings for categorical display\n",
    "\n",
    "random_columns = np.random.choice(data.columns, 2, replace=False)\n",
    "\n",
    "fig = px.scatter(\n",
    "    data, \n",
    "    x=random_columns[0], \n",
    "    y=random_columns[1], \n",
    "    color='Cluster',\n",
    "    title=\"DBSCAN Clustering Results\",\n",
    "    color_continuous_scale=px.colors.diverging.Tealrose,\n",
    "    labels={'color': 'Cluster ID'}\n",
    ")\n",
    "fig.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest plot\n",
    "\n",
    "show you at the meeting. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import Image\n",
    "from scipy.stats import randint\n",
    "\n",
    "# read data\n",
    "features_df = pd.read_csv('groups_ints.csv')  \n",
    "labels_df = pd.read_csv('groups_classes.csv') \n",
    "merged_df = pd.merge(features_df, labels_df, on='name')\n",
    "print(merged_df.info())\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "\n",
    "# prepare data\n",
    "X = merged_df.drop(['name','class'], axis=1)\n",
    "y = merged_df['class']\n",
    "\n",
    "# use oridalencoder\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_encoded = ordinal_encoder.fit_transform(X)\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 500],\n",
    "    'max_depth': [3, 20]\n",
    "    }\n",
    "\n",
    "# initialisiere and fit randomforestclassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train) # train the model\n",
    "y_pred_oe = rf.predict(X_test)\n",
    "\n",
    "# use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(estimator=rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# fit the random search object to the data\n",
    "print(\"Starting RandomizedSearch...\")\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)\n",
    "\n",
    "# create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# add accuracy\n",
    "accuracy_oe = accuracy_score(y_test, y_pred_oe)\n",
    "print(\"Accuracy: \", accuracy_oe)\n",
    "#print(\"Classification report:\\n\", classification_report(y_test, y_pred_oe))\n",
    "\n",
    "# create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_oe)\n",
    "cm_df = pd.DataFrame(cm, index=best_rf.classes_, columns=best_rf.classes_)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "\n",
    "# plot the confusion matrix using plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=cm_df.values,\n",
    "                    x=cm_df.columns,\n",
    "                    y=cm_df.index,\n",
    "                    colorscale='Blues',\n",
    "                    zmin=0, zmax=np.max(cm_df.values),\n",
    "                    colorbar=dict(title='Count'),\n",
    "                    hovertemplate='True Label: %{y}<br>Predicted: %{x}<br>Count: %{z}'\n",
    "                ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='True Label',\n",
    "    autosize=True,\n",
    "    xaxis=dict(tickmode='array', tickvals=np.arange(len(cm_df.columns)), ticktext=cm_df.columns),\n",
    "    yaxis=dict(tickmode='array', tickvals=np.arange(len(cm_df.index)), ticktext=cm_df.index)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = Source(dot_data)\n",
    "    display(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy the encoded x_train and x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from scipy.stats import randint\n",
    "\n",
    "# read data\n",
    "features_df = pd.read_csv('groups_ints.csv')  \n",
    "labels_df = pd.read_csv('groups_classes.csv') \n",
    "merged_df = pd.merge(features_df, labels_df, on='name')\n",
    "print(merged_df.info())\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "\n",
    "# prepare data\n",
    "X = merged_df.drop(['name', 'class'], axis=1)\n",
    "y = merged_df['class']\n",
    "\n",
    "# column to encode\n",
    "cols_to_encode = ['polarity', 'month', 'year','tag','monthclass']\n",
    "non_numerical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# use ordinalencoder\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_train_encoded = X_train.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "X_train[cols_to_encode] = ordinal_encoder.fit_transform(X_train[cols_to_encode])\n",
    "X_test[cols_to_encode] = ordinal_encoder.transform(X_test[cols_to_encode])\n",
    "\n",
    "# initialisiere and fit randomforestclassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred_oe = rf_classifier.predict(X_test)\n",
    "\n",
    "# use randomizedsearchcv\n",
    "param_dist = {'n_estimators': randint(50, 500), 'max_depth': randint(1, 20)}\n",
    "rand_search = RandomizedSearchCV(\n",
    "    rf_classifier, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=5, \n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "rand_search.fit(X_train, y_train)\n",
    "best_rf = rand_search.best_estimator_\n",
    "\n",
    "# see best hyperparameter and accuracy\n",
    "print(\"Best hyperparameters: \", rand_search.best_params_)\n",
    "accuracy_oe = accuracy_score(y_test, y_pred_oe)\n",
    "print(\"Accuracy: \", accuracy_oe)\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred_oe))\n",
    "\n",
    "# confusionsmatrix\n",
    "cm = confusion_matrix(y_test, y_pred_oe)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=best_rf.classes_, columns=best_rf.classes_)\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=cm_df.values,\n",
    "    x=cm_df.columns,\n",
    "    y=cm_df.index,\n",
    "    colorscale='Blues',\n",
    "    zmin=0, zmax=np.max(cm_df.values),\n",
    "    colorbar=dict(title='Count'),\n",
    "    hovertemplate='True Label: %{y}<br>Predicted: %{x}<br>Count: %{z}'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='True Label',\n",
    "    autosize=True\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = rf_classifier.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X_train.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = Source(dot_data)\n",
    "    display(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "from scipy.stats import randint\n",
    "\n",
    "# read data\n",
    "features_df = pd.read_csv('feature_list.csv')  \n",
    "labels_df = pd.read_csv('new_metadata.csv') \n",
    "merged_df = pd.merge(features_df, labels_df, on='name')\n",
    "print(merged_df.info())\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "\n",
    "# prepare data\n",
    "X = merged_df.drop(['name','class'], axis=1)\n",
    "y = merged_df['class']\n",
    "\n",
    "# use oridalencoder\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "X_encoded = ordinal_encoder.fit_transform(X)\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 500],\n",
    "    'max_depth': [3, 20]\n",
    "    }\n",
    "\n",
    "# initialisiere and fit RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train) # train the model\n",
    "y_pred_oe = rf.predict(X_test)\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(estimator=rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "# Fit the random search object to the data\n",
    "print(\"Starting RandomizedSearch...\")\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\",  rand_search.best_params_)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# add accuracy\n",
    "accuracy_oe = accuracy_score(y_test, y_pred_oe)\n",
    "print(\"Accuracy: \", accuracy_oe)\n",
    "#print(\"Classification report:\\n\", classification_report(y_test, y_pred_oe))\n",
    "\n",
    "# create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_oe)\n",
    "cm_df = pd.DataFrame(cm, index=best_rf.classes_, columns=best_rf.classes_)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "\n",
    "# plot the confusion matrix using plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=cm_df.values,\n",
    "                    x=cm_df.columns,\n",
    "                    y=cm_df.index,\n",
    "                    colorscale='Blues',\n",
    "                    zmin=0, zmax=np.max(cm_df.values),\n",
    "                    colorbar=dict(title='Count'),\n",
    "                    hovertemplate='True Label: %{y}<br>Predicted: %{x}<br>Count: %{z}'\n",
    "                ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='True Label',\n",
    "    autosize=True,\n",
    "    xaxis=dict(tickmode='array', tickvals=np.arange(len(cm_df.columns)), ticktext=cm_df.columns),\n",
    "    yaxis=dict(tickmode='array', tickvals=np.arange(len(cm_df.index)), ticktext=cm_df.index)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Export the first three decision trees from the forest\n",
    "for i in range(3):\n",
    "    tree = best_rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,\n",
    "                               feature_names=X.columns,  \n",
    "                               filled=True,  \n",
    "                               max_depth=2, \n",
    "                               impurity=False, \n",
    "                               proportion=True)\n",
    "    graph = Source(dot_data)\n",
    "    display(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hdbscan plot for the new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.print()\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name'])\n",
    "\n",
    "label_encoders = {}\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object': \n",
    "        le = LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "random_columns = np.random.choice(data.columns, 2, replace=False)\n",
    "\n",
    "clusterer = HDBSCAN(min_cluster_size=3)\n",
    "cluster_labels = clusterer.fit_predict(data)\n",
    "data['cluster'] = cluster_labels  \n",
    "\n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    x=random_columns[0],\n",
    "    y=random_columns[1],\n",
    "    color='cluster',\n",
    "    title='HDBSCAN Clustering mit Plotly'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('groups_ints.csv')\n",
    "data = df.drop(columns=['name'])\n",
    "\n",
    "label_encoders = {}\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object': \n",
    "        le = LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        label_encoders[column] = le  \n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42) \n",
    "cluster_labels = kmeans.fit_predict(data)\n",
    "\n",
    "pca_df = pd.DataFrame(data_pca, columns=['PCA1', 'PCA2'])\n",
    "pca_df['Cluster'] = cluster_labels.astype(str)  \n",
    "\n",
    "fig = px.scatter(pca_df, x='PCA1', y='PCA2', color='Cluster', title='KMeans Clustering Results',\n",
    "                 labels={'PCA1': 'Principal Component 1', 'PCA2': 'Principal Component 2'})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "data = df.drop(columns=['name'])  \n",
    "\n",
    "# Define number of clusters for KMeans\n",
    "n_clusters = 10  # Specify the number of clusters you expect\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "labels = kmeans.fit_predict(data)\n",
    "\n",
    "data['Cluster'] = labels.astype(str)  # Convert to string for color coding in plot\n",
    "\n",
    "random_columns = np.random.choice(data.columns[:-1], 2, replace=False)\n",
    "\n",
    "fig = px.scatter(\n",
    "    data, \n",
    "    x=random_columns[0], \n",
    "    y=random_columns[1], \n",
    "    color='Cluster',\n",
    "    title=\"KMeans Clustering Results\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    "    labels={'color': 'Cluster ID'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter 'month' with umap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'may')  \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"plot pos classes\")\n",
    "engine = MachineLearningEngine()  \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'april')  \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'april')  \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine, MachineLearningAnalyses\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelUMAP\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        row_value = row.tolist()[1:] \n",
    "        class_name = row['month']\n",
    "        ana = MachineLearningAnalyses(name=str(class_name), \n",
    "                                          data={\"x\": np.array(df.columns.tolist()[1:]), \n",
    "                                                \"y\": np.array(row_value)})\n",
    "        if ana.validate():\n",
    "            engine.add_classes(class_name)\n",
    "        else:\n",
    "            print(f\"Analyses {class_name} did not pass validation.\") \n",
    "\n",
    "umap_model = MakeModelUMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "engine.add_settings(umap_model)\n",
    "engine.run_workflow()\n",
    "engine.plot_umap() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter 'month' plot with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelPCASKL\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'may') \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "webbrowser.open('pca_scores_plot.html')\n",
    "webbrowser.open('pca_loadings_plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelPCASKL\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, 'december') \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 10, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "webbrowser.open('pca_scores_plot.html')\n",
    "webbrowser.open('pca_loadings_plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelPCASKL\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "class_path = 'groups_classes.csv'\n",
    "df = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine() \n",
    "engine.add_analyses_from_csv(path)\n",
    "\n",
    "engine.add_month_classes(df, ['may', 'april']) \n",
    "\n",
    "pca_model = MakeModelPCASKL(n_components = 2, center_data= True)\n",
    "engine.add_settings(pca_model)\n",
    "engine.print()\n",
    "engine.run_workflow()\n",
    "engine.plot_pca()\n",
    "webbrowser.open('pca_scores_plot.html')\n",
    "webbrowser.open('pca_loadings_plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca umap test for feautre list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "df_data = pd.read_csv(path)\n",
    "class_path = 'new_metadata.csv'\n",
    "df_classes = pd.read_csv(class_path)\n",
    "\n",
    "print(df_data.shape)\n",
    "print(df_classes.shape)\n",
    "\n",
    "df_merged = pd.merge(df_data, df_classes)\n",
    "print(df_merged.shape)\n",
    "print(df_merged.head())\n",
    "\n",
    "# standardize dataset\n",
    "df_data_numeric = df_merged.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_data_numeric)\n",
    "\n",
    "# the month with the most entries\n",
    "month_counts = df_merged['month'].value_counts()\n",
    "print(\"Anzahl der Eintr√§ge pro Monat:\\n\", month_counts)\n",
    "\n",
    "# unique months/classes\n",
    "unique_months = df_merged['month'].unique()\n",
    "unique_classes = df_merged['class'].unique()\n",
    "\n",
    "# find number of components\n",
    "nums = np.arange(1,20)\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "  pca = PCA(n_components=num)\n",
    "  pca.fit(data_scaled)\n",
    "  var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "df_var = pd.DataFrame({\"num\" : nums, \"var_ratio\" : var_ratio})\n",
    "fig = px.line(df_var, x='num', y='var_ratio', markers=True,\n",
    "              title=\"Explained Variance\",\n",
    "              labels={'num': 'Number of Components', 'var_ratio': 'Explained Variance Ratio'})\n",
    "fig.show()\n",
    "\n",
    "# pca\n",
    "n_components=10\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "print(principal_components.shape)\n",
    "\n",
    "pca_df = pd.DataFrame(data=principal_components[:, [0,1]],columns=['PC1', 'PC2'])\n",
    "pca_df['class'] = df_merged['class'].values  \n",
    "pca_df['month'] = df_merged['month'].values \n",
    "\n",
    "# get min/max for axis scaling\n",
    "offset = 5\n",
    "minC1 = pca_df.min()[0] - offset\n",
    "maxC1 = pca_df.max()[0] + offset\n",
    "minC2 = pca_df.min()[1] - offset\n",
    "maxC2 = pca_df.max()[1] + offset\n",
    "fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2', color='class',\n",
    "        facet_row='month',\n",
    "        title=f'PCA results by month',\n",
    "        labels={'PC1': 'Hauptkomponente 1', 'PC2': 'Hauptkomponente 2', 'color': 'class'},\n",
    "        range_x=[minC1, maxC1], range_y=[minC2, maxC2]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# loop through each class\n",
    "fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2', color='month', symbol='month',\n",
    "        facet_row='class',\n",
    "        title=f'PCA results by class',\n",
    "        labels={'PC1': 'Hauptkomponente 1', 'PC2': 'Hauptkomponente 2', 'color': 'month'},\n",
    "        range_x=[minC1, maxC1], range_y=[minC2, maxC2]\n",
    "    )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'feature_list.csv'\n",
    "df_data = pd.read_csv(path)\n",
    "class_path = 'new_metadata.csv'\n",
    "df_classes = pd.read_csv(class_path)\n",
    "\n",
    "df_merged = pd.merge(df_data, df_classes)\n",
    "df_data_numeric = df_merged.select_dtypes(include=[np.number])\n",
    "\n",
    "# standardize dataset\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_data_numeric)\n",
    "\n",
    "# use umap \n",
    "n_components = 2\n",
    "umap_model = umap.UMAP(n_components=n_components, random_state=42)\n",
    "umap_components = umap_model.fit_transform(data_scaled)\n",
    "\n",
    "umap_df = pd.DataFrame(data=umap_components, columns=['UMAP1', 'UMAP2'])\n",
    "umap_df['class'] = df_merged['class'].values\n",
    "umap_df['month'] = df_merged['month'].values\n",
    "\n",
    "fig = px.scatter(umap_df, x='UMAP1', y='UMAP2', color='month', title='UMAP f√ºr alle Monate',\n",
    "                 labels={'UMAP1': 'UMAP Komponente 1', 'UMAP2': 'UMAP Komponente 2'})\n",
    "fig.show()\n",
    "\n",
    "# find the month with the max entries\n",
    "month_counts = df_merged['month'].value_counts()\n",
    "max_entries_month = month_counts.idxmax()\n",
    "max_entries_count = month_counts.max()\n",
    "\n",
    "print(f\"Der Monat mit den meisten Eintr√§gen ist {max_entries_month} mit {max_entries_count} Eintr√§gen.\")\n",
    "print(\"Anzahl der Eintr√§ge pro Monat:\\n\", month_counts)\n",
    "\n",
    "\n",
    "# unique months\n",
    "unique_months = df_merged['month'].unique()\n",
    "\n",
    "# loop through each month\n",
    "for month in unique_months:\n",
    "    # filter data for the current month\n",
    "    month_data = umap_df[umap_df['month'] == month]\n",
    "    \n",
    "    # plot pca for current month\n",
    "    fig = px.scatter(\n",
    "        month_data, x='UMAP1', y='UMAP2', color='class',\n",
    "        title=f'UMAP 2D Scatter Plot f√ºr {month}',\n",
    "        labels={'color': 'class'}\n",
    "    )\n",
    "    fig.update_layout(xaxis_title=\"UMAP1\", yaxis_title=\"UMAP2\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test umap and pca for groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "from src.StreamPort.ml.MachineLearningProcessingMethod import MakeModelUMAP\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "df_data = pd.read_csv(path)\n",
    "class_path = 'groups_classes.csv'\n",
    "df_classes = pd.read_csv(class_path)\n",
    "\n",
    "engine = MachineLearningEngine()\n",
    "engine.add_analyses_from_csv(path)\n",
    "engine.add_classes_from_csv(class_path)\n",
    "engine.print()\n",
    "\n",
    "df_merged = pd.merge(df_data, df_classes)\n",
    "df_data_numeric = df_merged.select_dtypes(include=[np.number])\n",
    "\n",
    "# standardize dataset\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_data_numeric)\n",
    "\n",
    "# use umap \n",
    "n_components = 2\n",
    "umap_model = umap.UMAP(n_components=n_components, random_state=42)\n",
    "umap_components = umap_model.fit_transform(data_scaled)\n",
    "\n",
    "umap_df = pd.DataFrame(data=umap_components, columns=['UMAP1', 'UMAP2'])\n",
    "umap_df['class'] = df_merged['class'].values\n",
    "umap_df['month'] = df_merged['month'].values\n",
    "\n",
    "fig = px.scatter(umap_df, x='UMAP1', y='UMAP2', color='month', title='UMAP f√ºr alle Monate',\n",
    "                 labels={'UMAP1': 'UMAP Komponente 1', 'UMAP2': 'UMAP Komponente 2'})\n",
    "fig.show()\n",
    "\n",
    "# find the month with the max entries\n",
    "month_counts = df_merged['month'].value_counts()\n",
    "max_entries_month = month_counts.idxmax()\n",
    "max_entries_count = month_counts.max()\n",
    "\n",
    "print(f\"Der Monat mit den meisten Eintr√§gen ist {max_entries_month} mit {max_entries_count} Eintr√§gen.\")\n",
    "print(\"Anzahl der Eintr√§ge pro Monat:\\n\", month_counts)\n",
    "\n",
    "\n",
    "# unique months\n",
    "unique_months = df_merged['month'].unique()\n",
    "\n",
    "# loop through each month\n",
    "for month in unique_months:\n",
    "    # filter data for the current month\n",
    "    month_data = umap_df[umap_df['month'] == month]\n",
    "    \n",
    "    # plot pca for current month\n",
    "    fig = px.scatter(\n",
    "        month_data, x='UMAP1', y='UMAP2', color='class',\n",
    "        title=f'UMAP 2D Scatter Plot f√ºr {month}',\n",
    "        labels={'color': 'class'}\n",
    "    )\n",
    "    fig.update_layout(xaxis_title=\"UMAP1\", yaxis_title=\"UMAP2\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.StreamPort.ml.MachineLearningEngine import MachineLearningEngine\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "path = 'groups_ints.csv'\n",
    "df_data = pd.read_csv(path)\n",
    "class_path = 'groups_classes.csv'\n",
    "df_classes = pd.read_csv(class_path)\n",
    "\n",
    "print(df_data.shape)\n",
    "print(df_classes.shape)\n",
    "\n",
    "df_merged = pd.merge(df_data, df_classes)\n",
    "print(df_merged.shape)\n",
    "print(df_merged.head())\n",
    "\n",
    "# standardize dataset\n",
    "df_data_numeric = df_merged.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_data_numeric)\n",
    "\n",
    "# the month with the most entries\n",
    "month_counts = df_merged['month'].value_counts()\n",
    "print(\"Anzahl der Eintr√§ge pro Monat:\\n\", month_counts)\n",
    "\n",
    "# unique months/classes\n",
    "unique_months = df_merged['month'].unique()\n",
    "unique_classes = df_merged['class'].unique()\n",
    "\n",
    "# find number of components\n",
    "nums = np.arange(1,20)\n",
    "var_ratio = []\n",
    "for num in nums:\n",
    "    pca = PCA(n_components=num)\n",
    "    pca.fit(data_scaled)\n",
    "    var_ratio.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "df_var = pd.DataFrame({\"num\" : nums, \"var_ratio\" : var_ratio})\n",
    "fig = px.line(df_var, x='num', y='var_ratio', markers=True,\n",
    "              title=\"Explained Variance\",\n",
    "              labels={'num': 'Number of Components', 'var_ratio': 'Explained Variance Ratio'})\n",
    "fig.show()\n",
    "\n",
    "# pca\n",
    "n_components=10\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "print(principal_components.shape)\n",
    "\n",
    "pca_df = pd.DataFrame(data=principal_components[:, [0,1]],columns=['PC1', 'PC2'])\n",
    "pca_df['class'] = df_merged['class'].values  \n",
    "pca_df['month'] = df_merged['month'].values \n",
    "\n",
    "# get min/max for axis scaling\n",
    "offset = 5\n",
    "minC1 = pca_df.min()[0] - offset\n",
    "maxC1 = pca_df.max()[0] + offset\n",
    "minC2 = pca_df.min()[1] - offset\n",
    "maxC2 = pca_df.max()[1] + offset\n",
    "fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2', color='class',\n",
    "        facet_row='month',\n",
    "        title=f'PCA results by month',\n",
    "        labels={'PC1': 'Hauptkomponente 1', 'PC2': 'Hauptkomponente 2', 'color': 'class'},\n",
    "        range_x=[minC1, maxC1], range_y=[minC2, maxC2]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# loop through each class\n",
    "fig = px.scatter(\n",
    "        pca_df, x='PC1', y='PC2', color='month', symbol='month',\n",
    "        facet_row='class',\n",
    "        title=f'PCA results by class',\n",
    "        labels={'PC1': 'Hauptkomponente 1', 'PC2': 'Hauptkomponente 2', 'color': 'month'},\n",
    "        range_x=[minC1, maxC1], range_y=[minC2, maxC2]\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
